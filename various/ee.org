* Electrical Engeneering

** Silicon (Silizium)

1808 named by Sir Humphry Davy

from latin silix, silicis for flint and -ium for metal

1823 Jons Jocob Berzelius prepared amorohous silicon and is usually given
credit for discovering the element

Si
group: 14(carbon group), periodi: 3, Z=14
atomic radius: 111 pm (pico meters 1*10^-12)
Van der Waals radius: 210 pm
electron configuration: [Ne]3s^2 3p^2
electrons per shell: 2, 8, 4

8 most common in universe by mass
2 most common in the Earth's crust,
but rarely occurs as pure element, mostly as silicon dioxide(silica)

Used in steel refining, aluminium-casting, fine chemical industries, and small
portion of very highly purified silicon is essential to integrated circuits.


Monocrystaline silicon is used to produce silicon wafers and silicon oxide is
used as an insulator both in integrated circuits.

Polycrystaline silicon is used in photovoltaic

It is exelant semiconductor, can withstand highest temperatures and greatest
electrical activity without suffering avalanche breakdown(an electron avalanche
is created when heat produces free electron holes, which in turn pass more
current, which produces more heat). In addition the insulating silicon oxide is
easy to produce and is not soluble in water, which gives it great advantage
over other elements as germanium.

The entire manufacturing process, from start to packaged chips takes 6-9 weeks,
but as of 2017 regarding 14/10/7nm it can take up to 12-15 weeks (3-4 months).

** Transistors

basically an on/off switch

It has 4 electron on its outer layer and forms thethrahidrons

Doping - is injecting other elements into silicon to make it conductable

N Doping - injects element with more electrons like 5 so free electrons are
available

P Doping - injecting Phosphorus with 3 electrons so that holes(practicaly
positive charges) are left

Introducing voltage will make electicity flow or stop depending on the source
positionig + and -. This can be simulated with PNP transistor.

When we talk about the size of a transistor we mean the size of the
gate/barrier that passes or blocks the flow of electrons.

When it gets down to a few atoms the effects of quantum mechanics must be
considered. At that scale electrons might start to quantum tunnel through the
barrier, which makes for very unreliable transistor.

That can be initialy avoided by selecting elements with smaller atom radius,
but eventually the challenge is in engeneering a high enough barrier.

One other option is embracing quantum mechanics and using this behaviour to
build possibly quantum computers, which may be very well suited to certain type
of task (like breaking encryption or making it unbreakable).



** Limits to computaion

+-------+----+--------+
| milli | m  | 10^-3  |
| micro | mu | 10^-6  |
| nano  | n  | 10^-9  |
| pico  | p  | 10^-12 |
| femto | f  | 10^-15 |

Production in industry:

+---------+-------+
| 10  mun | 1971  |
| 1   mun | 1985  |
| 130 nm  | 2001  |
| 32  nm  | 2010  |
| 22  nm  | 2012  |
| 14  nm  | 2014  |
| 10  nm  | 2017  |
| 7   nm  | ~2018 |
| 5   nm  | ~2020 |

Concept demos in industry and research:

6nm transistor in 2002 by IBM
5nm transistor in 2003 by NEC
3nm transistor in 2006 by KAIST(Korea) and NNFC(US)

3x10 atoms transistor in 2008 by UK reseachers usinig graphene

7 atoms of 4nm transistor in 2010 by Australlian researchers

1 atom transistor 360 pn in 2012 was fabricated (first invented in 2004 Karlsruhe)

5nm test chips in 2015 bf IMEC and Cadence fabricated
5nm node FET concept in 2015 by Intel
1nm transistor gate in 2016 by Berkeley (MoS_2)

Note: check out Reversable Computation
It is trying to create a new way of physical and logical computing with very
tiny power usage.

input  2 + 2
output 4

during computation information is lost and inputs are no longer known
most energy is used




* Quantum Mechanics and Quantum Field Theory

** Quantum Tunneling

Is Quantum tunneling faster then light?

Certain properties of an object are fundamentaly uncertain.
They must be described as a distribution of possible states of being.
Each specific state has a certain probability of being true when the
object is observed.

Until a quantum object interacts with something
all possible states are just real as each other.
Although not necessaraliy equally likely.

There is a distribution of probabilities for each of an objects's quantum
properties. That distribution, and the way it changes over time, is coded
in the object's wave function. The reduction of a fuzzy posibility space
into a specific measurable property is referred to as the collapse of the
wave function.

Louis de Broglie(broie) figured out that any material object is really a
matter wave. It can be described as a wave packet of positioned probability.
And it has a wavelength - de Broglie wave length, that defines how well
determined an object's position is.
A large wavelength means a highly uncertain position, a small wavelength
means well-defined position. That's true of subatomic particles and it's
sort of true of anything.

"Observe me and collapse my wave function"

Objects wavelength depends on its momentum (mass times velocity).
Higher momentum means a smaller wavelength.
Its the minuscule Plank constant divided by momentum.

\lambda = h / p

Humans are made of several tens of kilograms of thermal moving particles
and have de Broglie wavelengths a couple of orders of magnitude smaller
than the Plank wavelength.

"You are everywhere in the universe, but not very much. You are as right
there as possible to be."

But what about something much smaller?

Say a tightly bound bundle of two protons and two neutrons that we call
an alpha particle. On it's own this would be a helium nucleus. But these
bundles also exist as parts of heavier atomic nuclei. There an alpha
particle is snugly bound into the nucleus by the strong nuclear force.

We can imagine an alpha particle as being like a ball trapped in a steep
valley of potential energy. It can roll around inside, but unless it has
a very large kinetic energy, it will never roll over the edges.

[[./img/potential_energy_valley.jpg]]

But quabtum objects aren't at al like balls. Their positions are not well
defined. As an alpha particle approaches the force barrier of nucleus,
its wave packet is reflected backwards, ... usualy. That wave packet
describes a range of possible locations for the approaching particle.
But that possibility space does not end suddenly at the force barrier.
Instead, it drops off quickly, exponentially, through the steep walls.
However it never quite reaches zero. There remains a tiny tail of
probability outside the nucleus, beyond the reach of the strong nuclear
force. That means there is a very tiny chance that instead of bouncing
off the wall, the particle will resolve its position in that unlikely
outside bit of its posibility space that looks like the particle
teleporting out of the nucleus.

This is called Quantum Tunneling:

[[./teleporting_out_of_nucleus.jpg]]

When it's an alpha particle is escaping a nucleus this is one of the
most important mechanisms for radioactive decay.

Quantum tunneling also goes in the other direction.

Protons, neutrons, electrons and alpha particles can quantum tunnel
into nuclei in various types of fusion and particle capture phenomena.
Without it stars could not fuse hydrogen into heavy nuclei.

A variety of modern electronics also rely on the tunneling phenomenon,
including the transistor.

But how quickly the particle moves through this barrier?

As far as we know it's instantaneous. That suggests a velocity faster
tha light, which sounds problematic. It's actually extreamly hard to
test this because we can't make clocks accurate enough to time such a
ridiculously quick event.

The LEGO interferometer that discovered gravitational waves.
Laser beams are sent down paths at right angles and then brought back
together. The photon wave packets interact with each other and produce
an interference pattern that is incredibly sensitive to differences in
path lengths.

[[./interferometer.jpg]]

If we change the arangement of beam?
We want to send individual photons instead of lesers. And we want to
block one of the paths with a very thin reflective barrier.

In the absence of quantum tunneling that barrier should reflect its
photon 100% of the time. But just like with the alpha particle, as the
photon approaches the barrier the wave packet defining its possible
location extends weakly beyond the barrier. About 99% of the time the
photon is reflected. But 1% of the time it will resolve itself beyond
the barrier and it will continue onits path.

If those rare tunneling photons really do travel instantaneusly through
the width of the barrier, then they should arrive at the detector slightly
ahead of the photon that travels the unimpeded path.

That will be apparent when their wave packets don't line up.

[[./tunneling_paths.jpg]]

For that to work you need to use a second, and perhaps even weirder
feature of quantum mechanics - quantum entanglement.
In order to produce these entangled states the path length of the
interferometer needs to be identical to very high precision.

Tune the path lengths until the weird effects of entanglement emerge,
and you know that they are equal. At that point, you can get an incredibly
precise measurement of any differences in photon travel time.

That experiment was already succesfully performed. They found the tunneling
photon does arrive a tiny bit earlier than its parther. It appears to
teleport through the barrier and so travel faster than light.

But this apparent violation of relativity only occurs deep within the
quantum realm.

A particle resolve its location anywhere within the vicinity of its
de Broglie wavelength. That uncertainty in location allows tunneling.
But even without barrier this location fuzziness leads to uncertainty in
the arrival time of the photon. An unimpeded photon could arrive at the
earlier time of the tunneling photon, because its wave packet includes
that in its range of possible positions. When you add the barrier, all
you are realy doing is reshaping the wave packet, selecting only the
posibility space of earlier arrival. This can look like an increase in the
speed of light, but only within the uncertainty range defined by the
de Broglie wavelength. Between the uncertainty range defined by the
Heisenberg uncertainty principle.

\delta x \delta p >= h / 2

Which is from where the de Broglie wavelength comes.

Any microscopic object is subject to a very well-defined speed limit.
But in the quantum realm, Heisenberg uncertainty does seem to allow
instantaneous motion, and even perhaps causality violation within the
quantum limits.


** Planck's Constant

The Planck's constant defines the size scale at which the familiar physics
of our macroscopic reality gives way to the weirdness of the quantum world.

The quantum behaivior of the microscopic is observable on all scales of the
universe. You can see the effect of this quantum behavior and even measure
the Planck's constant just by observing the color of sunlight.

Zeno's paradox is based on the assumption that space is infinitly divisible.
To overtake the tortous you need to travel to its previous position
infinite times.

But that is not true. As you distance to the tortois becomes unthinkably
small, there arises a quantum uncertainty in your location. It is imposible
to say whether your location is really behind or in front of the tortoise.

The Heisenberg uncertainty principle describes the smallest distance for
which an object's location can be meaningfully defined.

The tiny Planck constant, at 6.63 * 10^-34 J_s (Joule seconds) sets the
scale of the quantum blurriness.

"So it sort of defines a pixel scale to reality"

In many ways it defines the divisibility of the quantum world. The Planck
constant appears in all equations that describe quantum phenomena:

The Heisenberg Uncertainty principle

The de Broglie wavelength

The Schrodinger equation

The Energy Levels of Electron Orbits

The Relationship between Energy and Frequency of a Photon

It also sets the size of the Planck Length, which is, hypothetically the
length below which the concept of length loses meaning.

But it can be observed on our scale. It sets the color of sunlight.
If it were 25% smaller the sun would be violet.

Everithing in the universe glows with its own internal heat.
Heat is just the energy in the random motion of particles comprising
an object. Accelerated charges produce electromagnetic radiation - light.
And so an object made of jiggling charged particles glows. The hotter the
object is the faster its particles jiggle. And so the avarage frequency of
the resulting particles of light, of phothons, increases with temperature
and defines the color that we see.

Sun is yellow because it's 6000 Kelvin surface produces more photons in the
green yellow part of the elecromagnetic spectrum.
The blue superstar giant Rigel is 12000 Kelvin.
Human temperature is around ~310 Kelvin so your heat glow is mostly low
frequency infrared photons.

By the late 1800's the distribution of brightness with frequency produced
by hot objects had been mapped in careful experiments that blacked out
anything but glow of heat. The resulting blackbody spectrum looks like a
lopsided bell curve.

[[./blackbody_spectrum.jpg]]

However the deep physics bihind this shape remained a mystery. The key to
unlocking the mystery lay in finding a mathematical desription for the
blackbody spectrum.

Rayleigh and Jeans proposed the Equipartition Theorem.
An object's heat energy will end up juggling all of its particles in all
the ways that they can be juggled.
At equilibrium energy is evenly spread between all posible energy states.

The resulting Rayleigh-Jeans law described the blackbody spectrum
perfectly for low frequancy infrared light, but for higher frequency
like visible and ultravilet it predicted way too high values, actually
approaching infinity as frequancy increased.

B_u = u^2 * k * T / c^2

This was called the ultraviolet catastrophy. It meant that something
was fundamentally wrong with the classical physics that went into the
Rayleigh-Jeans law.

The problem turned out to be that in classsical physics everything can
be infinitely divided.
Their calculation allows particle to vibrate with any amount of energy,
all the way down to infinitesimally tiny wiggles. When they tried to
mathematically distribute heat energy to equipartitionates across
possible energy states, way too much energy got packed into the countless
very tiny energy states at high frequencies.

Max Planck resolved the catastrohe almost by accident.
He needed a math 'trick' to count the supposedly infinite energy states.
Out of frustration he just decided that those particles could only vibrate
with energies that were a multiple of some minimum energy. He quantized the
energy states. He set this minimum energy to be the frequency of a
particle's vibration times a very, very small number, a number that had yet
to be measured. It became the Planck constant.

[[./planck_constant.jpg]]


It limited how much energy those high frequency vibrations could hold and
decribed the shape of the blackbody spectrum exactly.

Planck's Law

B_u = 2hu / c^2 * 1 / e^(hu/kT) - 1

He expected the contant being just a math 'trick' to cancel out in the
final equation, but it didn't, it firmly entrenched in the law.

So energy quantization is real.

The constant had yet to be measured, so he just adjusted the value until
the law matched the observed spectrum.

Later Einstein realized that it is actually light that is quantized.

Those little vibrating particles do have quantized energies, but it is
because they can only gain or lose energy by absorbing or emitting one
particle of light at a time. And that light comes in indivisible energy
packets.

[[./energy_packets.jpg]]
[[./energy_packets_photon.jpg]]

Planck's discovery was the clue Einstein needed to hypothesize the
existence of the photon - part wave, part particle carrying a quantum
of energy equal to the now familiar frequency of the wave times the
Planck constant.

These discoveries let to the quantum revolutioin of the 1920's.

By defining the shape of the blackbody spectrum the Planck constant can be
read in the color of the sun and the stars, in the brightness of the
different colors of the rainbow. And combined with a small handful of other
fundamental constants, it governs the behavior of everything in the
space time.

** The Single Particle Double-slit Experiment

One of the strangest experiments ever observed.
Illustraits how the quantum world is very different from the large scale
world of our physical intuition. In fact, it hints that the funcdamental
nature of reality may not be physical at all.

A rubber duckie bobs up and down in a pool, causing periodic ripples to
spread out. Some distance away, rhose waves encounter a barrier with two
gaps cut in it. Most of the wave is blocked, but ripples pass through the
gaps. When the new ripple start to overlap each other, they produce a
pattern, called the interferece pattern.

It is due to the fact in some places the peak of the ripple from one gap
stacks on top of the peak of the other gap, making more extream peak
and more extream dips when two dips overlap.
We call this constructive interference.

[[./constructive_interference.jpg]]

But when the peak from one wave encounters the trough from another, they
cancel out, leaving nothing - destructive interference.

So we have these alternating tracks of wavy and flat water.
Any type of wave should make interference pattern like this.

A source of light passing through two very thin slits produces bands of
light and dark stripes, alternating regions of constructive and destructive
interference.

We know that light is a wave in the electromagnetic field (Maxwell).

But we also know that light comes in indivisible little bundles of
electromagnetic energy called photons (Einstein).

So each photon is a little bundle of waves, waves of electromagnetic
field, and each bundle can't be broken into smaller parts. That means
that each photon should have to decide whether it's going to go through
one slit or the other. It can't split and then recombine.

[[./laser_emitter.jpg]]

That shouldn't be a problem as long as you have at least two photons.
But here we get to one of the craziest experimental results in physics.

The interference pattern is seen even if you fire those photons one
at a time.

This pattern has nothing to do with how each photon's energy gets spread
out, as was the case with the water wave. Each photon dumos all of its
energy at a single point. The pattern emerges in the distribution of final
positions of many completely unrelated photons.

Each photon has no idea where previous photons landed or where future
photons will land, yet each photon reaches the screen knowing which
regions are the most likely landing spots and which are the least likely.
It knows the interference pattern of a pure wave that passed through both
slits equally and it chooses its landing point based on that.

[[./interference_distribution.jpg]]

Electrons, whole atoms and even whole molecules (buckminsterfullerine a
molecule of 60 carbon atoms) build up the same sort of interference
pattern. We have to conclude that each individual electron, atom or
buckyball travels through both slits as some sort of wave. That wave then
interact with itself to produce an iterference patterns. Except that here
the peaks of that pattern are regions where there is more chance that the
particle will find itself. It looks like a wave of possible undefined
positions that at some point for some reason, resoves itself into a single
certain position. We can also see this waviness in position in quantum
tunneling.

Several quantum properties like momentum, energy, and spin, all display
similar waviness in different situations. We call the mathematical
description of this wave-like distribution of properties a "wave function".

Describing the behavior of the wave function is the heart of quantum
mechanics.

But what does the wave function represent?

We know where the particle is at both ends. It starts its journey at the
"emitter" andreleases its energy at a well-defined spot on the screen.
So the particle seems to be more particle-like at either end, but wave-like
in between. That wave holds the information about all the possible final
positions of the particle but also about its possible positions at every
stage in the journey. In fact the wave must map out all possible paths
that the particle could take. We have this family of could-be trajectories
from start to finish and for some reason, when the wave reaches the
screen, it chooses a final location and that implies choosing from these
possible paths.

Within that misterious span between the creation and the detection, is the
particle anything more than a space of possibility?

In fact the answers aren't known.

But the various interpretations of quantum mechanics do try.


** The Copenhagen Interpretation

The Copenhagen interpretation is a view favoured by Heisenberg anf Bohr.
The wave function doesn't have a physical nature. Instead its comprised
of pure possibility. A particle traversing the double-slit experiment
exists only as a wave od possible locations that ultimately encompasses
all possible paths. It's only when the particle is detected that a
location and the path it took to get there are decided.

They call this transition from possibility space to a defined set of
properties "the collapse of the wave function". It tells us that prior to
the collapse, it's meaningless to try to define a perticle's properties.
It is almost like the universe is allowing all possibilities to exist
simultaneously but holds off choosing which actually happedned until
the last instant. Weirder this possible realities interact with each other.
That interaction increases the chance that some paths become real and
decreases the chance of others. There's an interaction between possible
realities that is seen in the distribution of final positions.

The interference pattern is real, even though the vast majority of paths
involved in producing the interference never attain reality.

That final choose of the experiment of the universe is fundamentally random
within the constraints of the final wave function.

The theory of quantum mechanics produces stunningly accurate predictions of
reality and it is completely consistent with the Copenhagen interpretation.

But this is not the only interpretation that works. There are
interpretations that give the wave function a physical reality.


** The Many Worlds Interpretaion

** Quantum Erasers

Can reality be adjusted after events have occurred?

This is the unsettling implication of the delayed choice quantum eraser
experiment.

Which Way experiment

What if we try to detect through which slit each particle actually travels
throygh before they produce the famous interference pattern.
It turns out that any experiment that determines unambiguously which slit
the particle traverses destroys the pattern.
Instead particles land in simple clumps, one for each slit, as they were
traveling as simple particles the whole time. This is even true if you
place detectors on the far side of the slits after the wave partice thing
should have already been interfering with itself, just like the wave
function is collapsing retroactively.

It is impossible to make this measurments without messing up the wave.
The interference pattern happens because the waves emerging from each slit
are what we call coherent, which means that the relationship between the
wave form is emerging from the two slits.

So the locations of peaks and valleys is predictable and stays consistent
as the waves move forward. But when you place some device in the path of
eithe wave, you mess with this coherence and so sffecr the pattern that
reaches the screen.

A Delayed Choice Quantum Eraser experiment

This experiment made use of a very special type of crystal that absorbs an
incoming photon, and creates two new photons, each with half the energy of
the original. These photon are twins and an entangled pair.

[[./quantum_eraser_a.jpg]]

Place this crystal in front of double slit to make coherent entangled pairs
of any photons passing through. Send one of each pair off to the screen to
produce our interference pattern and use the other to figure out which slit
the original photon passed through.

Let's focus on detectors A and B.
Detector A lights up if the original photon passed through slit A. And
detector B lights up for slit B. If we run this for a bunch of photons,
we see that whenever detectors A or B light up, we get a simple pile of
photons here at the screen. No interference pattern at all.
As though any knowledge if which way the original photon traveled stops
it from acting like a wave during its passage through the slits.
And crazier this experiment was set up so that photons reach A or B after
their twins reach the screen.
So a photon lands on the screen to the pattern defoned bt its wave function.
And then later, its untangled partner reaches detector A or B, and somehow
retroactively influances the previous landing position. It's like the
second photon is saying, whoa, whoa someone figured out which slit I came
through, you better look like you came through that one, too.

But it gets even weirder. The extra C and D are the quantum eraser.
Its job is to destroy any information about the path of the photons
by using beam splitters (half-silvered mirrors) just before A and B.
They allow 50% of the photons through, while reflecting the other 50%.
Now you have a new possible outcome. Instead of being reflected to
detectors A or B, half of the photons end up in detectors C or D.
But this assuares that if C or D light up, we have no idea which slit that
photon came from. If we only look at the photons whose twins end up at
detector C or D, we do see an interference pattern. It looks like the
simple act of scrambling the "Which Way" information retroactively sends
the message: "OK. Chill the observer lost the info of which slit we went
through. It's safe to have gone through both again." It looks like some
sort of retroactve reality cascade. But better be cautions.

Part of the appeal of the Copenhagen interpretation is that it avoids
any physical interaction that moves faster than light. When a spread
out wave function resolves itself into a set of known properties, the
location of a particle om the double slit screen, somehow the entire
wave function knows to do this - to collapse at the same instant. But
if these wave functions are physical, then there is no real instantaneous
physical interaction.

By contrast a physical interpretation of the wave function, like the
de Broglie-Bohn pilot wave theory, requires an underlying physicality, a
set of defines properties that evolves with the wave function.

So-called hidden variables. That's unconftoble, vecause these physical
properties need to act and change instantly at any distance.

They need to have what we call non-locality.

The delayed choice double slit experiment doesn't tell us wheter the wave
function is physical or not.

The solution may lie in the facinating phenomenon of the quantum
entanglement. Enatngled particles are really able to influence each other
instantaneously and their non-locality doesn't violate causality.
So perheps they can even affect coherence and decoherence retroactively
and physically without making a causal mess.

Perhaps this rhing we call observation is just entanglement between the
observer and the experiment. Perhaps the evolving tapestry of
entanglement in all its impossible complexity is what really defines
reality.


** Quantum Entanglement

Object permanence (the peekaboo game) is so deeply embeded both in our views
and classical physics that we never quation it.
Yet the idea that the universe keeps existing when we are not looking at it,
is a pretty fundamential implied assumption.
This notion that the universe exists independent of the mind of the observer
is called realism in physics.


** NOTES

*** Physics of Life

To understand life we need to understand Entropy. The universe tends toward
disorder, decay and equilibrium. A hot cup of coffe will tend towards the same
temperature as the room, and the hot, dense of our universe must expand.
Starts always burn out, black holes eveporate. The particles that make up any
system all have some degree of random motion. That random motion tends to drive
the system towards the most common arrangement of particles. Such a random
disordered unspecial arrangement is a high entropy state. Interesting
arrangenments like thermal energy being concentrated in your cup of coffe or
all the matter in the observable universe being crunched into an infiniteky
dense point are low entropy. They are highly specific configurations that
almost never happen by chance. So entropy is sort of a measure of the
boringness of a system, the commonness of the arrangement of particles.
The second law of thermodynamics tells us that a closed system will only
increase in entopy. The universe will only get more boring. But there is one
type of system that seems to resist the second law of thermodynamics and
maintain low entropy. That system is life. Life has a very low internal entropy
because its structure is extremely specific and non-random. The molecular
machinery of even a single cell defies belief. Not only is life stunningly
complex, but that complexity increases over extremly long time scales, in fact
over eons. Naively, this presentation and increase in order appears to
contradict the second law of thermodynamics entropy appears to either stay
constant or decrease. The Earth's biosphere, at least, becomes less boring
over time. But there is no violation of the second law. It tells us that
'closed' systems must increase in entropy. So a system's unable to exchange
energy with the outside environment. But living organisms and indeed the
Earth's biosphere are not closed. Both receive energy from outside.
Ultimetly, that source of energy is the sun. Its light warms the atmosphere
in the oceans and powers photosynthesis at the bottom of the foos chain,
driving a complex chain of nutrient synthesis that ends with whatever you had
for dinner last night. On the other hand the system  of the Earth plus the Sun
is encreasing in entropy. Life acts to reduce its own internal entropy by
increasing the entropy of its surroundings. This was first pointed out by
Ludwig Boltzmann, who described life as a struggle for entropy, more accuratly
agains entropy, or for negative entropy. Erwin Schrodingen, in his 1944 book
"What is Life", describes life as a process feeding on negative entropy.
Life absorbs order and it ejects disorder into its surroundings. The type of
order that life feeds on can be thougth of as free energy. By free energy, we
understand special out-of-equilibrium energy sources like a cuo of coffee or
the sun. Another way to say this is that life feeds on energy gradients.
When two systems with very different energy densities come into contact,
energy must flow. Life feeds on that flow. In fact the importance of energy
gradients to life can help us understand the actual origin of life and its
precursors. The origin of life on Earth isn't known. We think it started
with a  self-replicating molecule similar to RNA. Following that synthesis,
evolution took hold, and the first protocell and then first true living cell
pulled itself together. LUCA the last universal common ancestor. But where on
Earth that happened. Perhaps it was in tidal pools or around deep sea
hydrothermal vents or even on the undersurface of Earth's ice caps. These
environments share a critical property. Thay sit at persistent energy
gradients. The water of tidal pools is both cooled by the Earth and the ocean
and warmed by the sun. Around deep see vents, the searing gases from Earth's
hot interior meet the frigit water of the ocean depths. Beneath the thick ice
caps, there is the transition between the solid and liquid phases of water.
These are places struggling to return to equilibrium. These systems are doing
their best to obey the second law of thermodynamics by redistributing their
energy as evenly and randomly as they can. Heat energy fows from hot to cold,
seeking a uniform temperature, but energy is also dispersed into every form it
can take consistent with the laws of physiscs. Some of that energy gets
distributed into chemical bonds as simple molecules form via every chemical
reaction that's possible given the available raw materials. As those molecules
form, new channels open up for distributing energy into the chemical bonds
of increasingly complex molecules. Normally, this local rise in complexity
would all cease when the system reaches thermal equilibrium, energy is
perfectly evenly distributed and new molecules break apart exactly as often as
they are formed. But when our energy source is flowing into a much larger
reservoir, why, the oceans, for example then equilibrium is never reavhed.
Complexity can increase indefinitly as a byproduct of the system striving to
redistribute the endless gradient in energy. And at some point, natural
selection takes over. Molecules self-catalyze. They help drive the very
reactions that create more of the same. Molecules better at that process become
more abundant, and at some point, they become true self-replicators and
eventually, they become life. But even life and self-replication might be a
very natural part of the same thermodynamic drive to dissipate energy.
If you think about it, living things are incredible heat dissipation,
entropy-maximizing machenes. The most random possible form for energy
is thermal radiation, and the lower the energy of its component photons,
the higher the entropy.

A plant absorbs the concentrated ultraviolet light from the sun and reprocesses
it into a much higher entropy infrared heat glow. Animals consume high-energy
density packats of matter called food and convert it to lower energy density
waste as well as that same infrared heat glow. Life is great at dissipating
energy, and more generally, it may be that self-replicating systems are the
best possible energy dissipators of all. This is a new idea proposes by MIT
biophysicist Jeremy England ("Statistical Physics of Self-Replication"), who
puts the thermodynamics of life on more solid theoretical grounds. He's
demonstrated mathematically that self-replicating molecules and simple-cell
life are extreamly good at shredding heat in the act of reproduction.
Self-replication randomizes the environment, even if each new replicator is
highly ordered. And it is not just life that does this. Consider a perfectly
streamlined or laminar flow of some fluid. This organized flow is disrupted
by introducing turbulence. The laminar flow has a lower entropy than the
turbulent flow because there are fewer ways to rearrange the particles in the
former while preserving its global properties. The transition from laminar to
turbulent, while the global structure is disrupted, substructure develops.
Waves and vortices have their own complex and regular structures, but they
ultimately serve to dissipate the flow. Any given eddy taken separately has a
lower internal entropy than its chaotic surroundings, but the source of that
local incidence of low entropy is the streamline flow that it formed in.
And those turbulant eddies ultimately serve to increase the entropy of the
greater flow. So diven a much larer source of order, the global process of
dissipation of that order results in eddied of low entropy. Life appears to be
just such an eddy. In the case of life, the original source of extreame low
entropy is the Big Bang itself. In the process of redistributing energy into
the most random possible state, little eddies of order, like galaxies, starts,
planets, and life naturally arise. These blips in order are actually serving
the second law helping the universe disperse its early extreame low entropy
state. So everyone is a little eddy of roder, a momentary fluctuation of
interesting but ultimately, in service of the spread of disorder and dullness,
an agent in the inexorable trend to maximaze the entropy of space-time.



* EE from khanacademy

** Intro

*** Current

The story of current starts with the idea of charges, positive and negative
charge. If they are opposite sign there will be force of attraction between
them, or else repel them.

- +

< + + >

- > < +

We can get charge from copper and copper wires.

Cu 29 (29 protons inside and 29 electrons outside)

The last orbit has just one electron in it and it is easiest to pull away
and have it go participate in conduction (electric current).

e-

Silver has the same kind of electron configuration.

If we hook up a source of voltage (battery) to a copper wire, all the loose
last orbit electron will start moving in the direction of the + source on
the battery and at the end an electron will come out the - side of the
battery and make up the difference.

But how do we measure what is heppaning?

count how many e- are passing through a cut in the wire for a second.

Current is reported as the number of charges per unit time passing through
a boundary.

Current i = charge-/sec

Water and Salt + battery experiment

H2O and Na+Cl-

[[./img/current.jpg]]


What is the direction of Current?

positive current direction is opposite to the electron flow

negative current direction is the direction of the electron flow

Ben Franklin make the decision that the positive current is the direction
of current (nobody knew anything about electrons back then).

Note:
Electron is discovered by J.J.Thompson in 1897

Conventional Current is this positive direction current.

*** Voltage

Intuition:

As idea it is very similar to Gravity.

If you have a large body of mass on the slopes of a mountain, the potential
energy it has is going to be dissipated as kinetic energy and it is going to
roll down the hill.

We can think as voltage being a moutain top and battery delivering e- to the
top and if you release it it goes through all kinds of capacitors, resistors
etc. to the + bottom. High mountain high voltage.

We disign circuits by putting stuff in the way of the electron current

[[./img/volatage_intuition.jpg]]


** Circuit Analysis
*** Ideal Circuit Elemements

Resistor R

v = i * R, also known as Ohm's Law

Capacitor C

i = C dv/dt
C is proportionality canstant

Inductor L

v = L di/dt
L proportionality constant is the inductance of the inductor,
time rate of change of current flowing through the inductor

[[./img/ideal_circuit_elements.jpg]]

Ideal Sources

Constant Voltage source
Constant Current source

[[./img/ideal_sources.jpg]]
