{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus of Backpropagation\n",
    "\n",
    "Lets start with an extreamly simple network. Each layer has just one neuron in it.\n",
    "This particular network is determined by 3 weigths and 3 biases, and our goal is to \n",
    "understand how sensitive the Cost Function is to these variables.\n",
    "\n",
    "$$ C(w_1, b_1, w_2, b_2, w_3, b_3) $$\n",
    "\n",
    "Lets label the activation of the neurons as:\n",
    "\n",
    "$ a^{(L-3)} \\to a^{(L-2)} \\to a^{(L-1)} \\to a^{(L)} \\to y $\n",
    "\n",
    "where $ y $ is the desired output:\n",
    "\n",
    "![extreamly simple nn](img/extreamly_simple_nn.jpg)\n",
    "\n",
    "So the cost of this simple network for a single training example is:\n",
    "\n",
    "$ C_0(\\dots) = (a^{L} - y)^2 $\n",
    "\n",
    "where:\n",
    "\n",
    "$ a^{(L)} = \\sigma(z^{(L)}) $\n",
    "\n",
    "$ z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)} $\n",
    "\n",
    "All of these are numbers and we can think of them as each having it own number line.\n",
    "Our first goal is to understand how sensitive is the Cost Function to small changes\n",
    "in $ w^{(L)} $. In other words what is the derivative of C with respect to $ w^{(L)} $.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(L)}} $$\n",
    "\n",
    "Think of the $ \\partial w^{(L)} $ term as a tiny nudge to $ w^{(L)} $, like a change of 0.01.\n",
    "And the $ \\partial C $ term is the resulting nudge to the cost. What we want is their ratio.\n",
    "Conceptually this tiny nudge to $ w^{(L)} $ causes nudge to $ z^{(L)} $, which in term causes\n",
    "some nudge to $ a^{(L)} $, which directly influences the cost. Multipling together those\n",
    "3 ratios gives us the sensitivity of $ C $ to small changes in $ w^{(L)} $.\n",
    "\n",
    "![chain rule](img/chain_rule.jpg)\n",
    "\n",
    "Now we have to find the derivatives for each.\n",
    "Derivative of:\n",
    "\n",
    "$ C_0 = (a^{(L)} - y)^{2} $\n",
    "\n",
    "is:\n",
    "\n",
    "$ \\frac{\\partial C}{\\partial a^{(L)}} = 2(a^{(L)} - y) \\frac{\\partial C}{\\partial a^{(L)}} [a^{(L)} - y] = 2(a^{(L)} - y) $\n",
    "\n",
    "This means that its size is proportional to the difference between the network's output and the\n",
    "thing we want it to be ($ y $). So if that output was very different, even slight changes stand\n",
    "to have big impact on the Cost Function.\n",
    "\n",
    "Derivative of:\n",
    "\n",
    "$$ a^{(L)} = \\sigma(z^{(L)}) $$\n",
    "\n",
    "For simplicity we assume the activation function is a sigmoid function, but as well it may be a\n",
    "different function for example a ReLU and than the dervation is specific to the derivative of\n",
    "that activation function:\n",
    "\n",
    "$$ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\sigma'(z^{(L)}) = \\frac{e^{-x}}{1 - e^{-z^{(L)}}} $$\n",
    "\n",
    "Derivative of:\n",
    "\n",
    "$$ z^{(L)} = b^{(L)} + a^{(L-1)}w^{(L)} $$\n",
    "\n",
    "is just:\n",
    "\n",
    "$$ \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)} $$\n",
    "\n",
    "Note that when we put it all together we get just the derivative with respect to $ w^{(L)} $\n",
    "only of the cost for a specific training example:\n",
    "\n",
    "$$ \\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C}{\\partial a^{(L)}} = a^{(L-1)} \\sigma'(z^{(L)}) 2(a^{(L)} - y) $$\n",
    "\n",
    "Since the whole Cost Function envolves averaging together all costs across many training examples,\n",
    "its derivative requires avaraging this expression that we found over all training examples.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(L)}} = \\frac{1}{n}\\sum_{k=0}^{n-1}\\frac{\\partial C_k}{\\partial w^{(L)}} $$\n",
    "\n",
    "And that is just one component of the gradient vector, which itself is build from the partial\n",
    "derivatives of the Cost Function with respect to all the weights and biases:\n",
    "\n",
    "$$ \\nabla C = \\begin{bmatrix} \\frac{\\partial C}{\\partial w^{(1)}} \\\\ \\frac{\\partial C}{\\partial b^{(1)}} \\\\ \\dots \\\\\n",
    "\\frac{\\partial C}{\\partial w^{(L)}} \\\\ \\frac{\\partial C}{\\partial b^{(L)}} \\end{bmatrix} $$\n",
    "\n",
    "Now we need to do the same for the sensitivity to the biases. We just need to change out\n",
    "the $ \\frac{\\partial C}{\\partial w^{(L)}} $ term for a $ \\frac{\\partial C}{\\partial b^{(L)}} $\n",
    "term.\n",
    "\n",
    "$$ z^{(L)} = b^{(L)} + a^{(L-1)}w^{(L)} $$\n",
    "\n",
    "And from the formula its derivative comes to be just 1:\n",
    "\n",
    "$$ \\frac{\\partial C_0}{\\partial b^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial b^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C}{\\partial a^{(L)}} = 1 \\sigma'(z^{(L)}) 2(a^{(L)} - y) $$\n",
    "\n",
    "Also this is where the idea of propagating backwards come. You see how sensitive the Cost Function\n",
    "is to the activation of the previous layer. Namely the initial derivative ($ \\frac{\\partial z^{(L)}}{\\partial w^{(L-1)}} $) in the chain rule expression, the sensitivity of z to the previous activation,\n",
    "comes out to be the weight w^{(L)}. And even we won't be able to influence that previous layer\n",
    "activation, it's helpful to keep track of it, because now we can just keep iterating the same chain rule\n",
    "idea backwards to see how sensitive the Cost Function is to previous weights and to previous biases.\n",
    "\n",
    "When we scale up to a more complex, realistic example it does not get much more complicated.\n",
    "We just need to add and keep track of few more indeces. Rather than the activation of some\n",
    "layer be just $ a^{(L)} $ its going to have a subscript indicating which neuron of that layer it is.\n",
    "\n",
    "$ a_{0}^{(L-1)} ;  a_{1}^{(L-1)} ; a_{2}^{(L-1)} $\n",
    "\n",
    "are all activations from neurons in the $ L-1 $ layer.\n",
    "\n",
    "Lets use the letter $ k $ to index the layer $ L-1 $ and the letter $ j $ to index the layer $ L $.\n",
    "For the Cost we look at what the desired output is, but this time we add up the squares of the\n",
    "differences between these last layer activations and the desired output.\n",
    "\n",
    "$$ C_{0} = \\sum_{j=0}^{n_{L}-1} (a_{j}^{(L)} - y_{j})^{2} $$\n",
    "\n",
    "Lets call the weight of the edge connecting the k-th neuron to the j-th neuron:\n",
    "\n",
    "$ w_{jk}^{(L)} $\n",
    "\n",
    "![multiple layered nn indexed](img/multiple_layer_nn_indexing.jpg)\n",
    "\n",
    "The relevant weighted sum is now:\n",
    "\n",
    "$$ z_{j}^{(L)} = w_{j0}^{(L)}a_{0}^{(L-1)} + w_{j1}^{(L)}a_{1}^{(L-1)} + w_{j2}^{(L)}a_{2}^{(L-1)} + b_{j}^{(L)} $$\n",
    "\n",
    "The activation of the last layer is just the activation function applied to $ z $:\n",
    "\n",
    "$$ a_{j}^{(L)} = \\sigma(z_{j}^{(L)}) $$\n",
    "\n",
    "The chain rule derivative expression looks essentially the same:\n",
    "\n",
    "$$ \\frac{\\partial C_0}{\\partial w_{jk}^{(L)}} = \\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}} \\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}} \\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}} $$\n",
    "\n",
    "What changes here is the derivative with respect to one of the activations in the layer $ L-1 $.\n",
    "In this case the difference is the neuron influences the Cost Function through multiple different\n",
    "paths. On one hand it influences $ a_{0}^{(L)} $, which plays a role in the Cost Function, but\n",
    "it also has an influence on a_{1}^{(L)}, which also plays a role in the Cost Function and you\n",
    "have to add those up:\n",
    "\n",
    "$$ \\frac{\\partial C_0}{\\partial a_{k}^{(L-1)}} = \\sum_{j=0}^{n_{L}-1} \\frac{\\partial z_{j}^{(L)}}{\\partial a_{k}^{(L-1)}} \\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}} \\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}} $$\n",
    "\n",
    "Once you know how sensitive the Cost Function is to the activations in this second to last layer\n",
    "you can just repeat the process for all the weights and biases feeding into that layer.\n",
    "\n",
    "These chain rule expressions give you the derivatives that determine each component in the\n",
    "gradient that helps minimize the cost of the network by repeatedly steping downhill.\n",
    "\n",
    "![summary](img/summary.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
