* Lecture 01: Intro to ML

* Lecture 02: Linear and Logistic Regression

* Lecture 03: Python

** Conditionals

if cond1:
    do()
ifel cond2:
    do()
else:
    do()

** Loops:

>>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
>>> numbers = [1, 2, 3, 4, 5, 6, 7, 8]

>>> for k, v in knights.items(): print(k, v)

>>> for x in numbers: print(x)

>>> for i in range(10): do()


** Collections:

   Seqs are mutable(list) and immutable(tuple, range)

*** tuple

>>> t = (1,2,3,4)

    immutable,
    collection
 
    - packing and unpacking

>>> t = 12345, 54321, 'hello!'
>>> x, y, z = t


*** list

>>> l = [1,2,3,4]

    mutable,
    more like array or vector or linked-list?
    
    - methods:
      .append(x)                - adds at the end
      .extend(iterable)
      .insert(i, x)
      .remove(x)                - remove first by value
      .pop([i])                 - if no arg, returns last
      .clear()
      .index(x[,start[,end]])   - return 0-based index of value
      .count
      .sort
      .reverse
      .copy
      del a[i:j]                - removes by index at pos or range

    - as stacks:
      with .append() and .pop()

    - as queues:
      with .append() and .pop(0)

    - list comprehensions:

>>> [x**2 for x in range(10)]
>>> [x for x in vec if x >= 0]
>>> [(x, x**2) for x in range(6)]

>>> # flatten a list using a listcomp with two 'for'

>>> vec = [[1,2,3], [4,5,6], [7,8,9]]
>>> [num for elem in vec for num in elem]

Out[]: [1, 2, 3, 4, 5, 6, 7, 8, 9]

    - queues: better use collections deque

>>> from collections import deque
>>> queue = deque(["Eric", "John", "Michael"])
>>> queue.append("Terry")           # Terry arrives
>>> queue.append("Graham")          # Graham arrives
>>> queue.popleft()

    - map:

>>> squares = list(map(lambda x: x**2, range(10)))

    - reduce:

>>> 

    - filter:

>>> 


*** dict

>>> tel = {'jack': 4098, 'sape': 4139}

    classical hash map

    - methods:
    d[key]                 - to access by key
    del d[key]             - remove
    .iter(d)               - return an iterator over the keys of the dictionary
    .clear()
    .copy()
    .get(key[,default])    - get value else default value
    .keys()
    .values()
    .items()
    .pop(key[,default])
    .popitem()             - remove arbitrary
    .update([other])       - overwrites existing 

>>> dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])
>>> dict(sape=4139, guido=4127, jack=4098)

*** set

>>> basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}

    like math sets

>>> 'orange' in basket
Out[]: True

>>> a = set('abracadabra')
Out[]: {'a', 'r', 'b', 'c', 'd'} # unique letters in a


*** range

>>> range(1,10)

    range(start, stop, step)
    
    it is useful, because it is more efficient. Values are generated only when needed
    lazy seq

*** Common Seq Operations

    x in s 	              True if an item of s is equal to x, else False
    x not in s 	          False if an item of s is equal to x, else True
    s + t 	              the concatenation of s and t
    s * n or n * s 	      equivalent to adding s to itself n times
    s[i] 	                ith item of s, origin 0
    s[i:j] 	              slice of s from i to j
    s[i:j:k] 	            slice of s from i to j with step k
    len(s) 	              length of s
    min(s) 	              smallest item of s
    max(s) 	              largest item of s
    s.index(x[, i[, j]]) 	index of the first occurrence of x in s (at or after index i and before index j)
    s.count(x)

** Functions:
    
- argument lists
- keyword arguments
- default arguments

- destructuring

a,b,c = abc(): return a, b, c


- recur
   no tco!?


** Decorators

** OOP
    
** Generators
   
* Lecture 04:   

* Lecture 05:

* Lecture 06: ... data exploration, feature enfineering



* Andrew Ng Machine Learning at Coursera

** Week 01.

*** Definition

    Example with checkers:
    E ::= the experience of playing many games
    T ::= the task of playing the game
    P ::= the probability of winning the next game

    if P to do T increases as E increases it is machine learning
    
    Probability Task Experience 
    
*** Supervised learning

    Task: Housing price prediction
    
    y price, x size

    Straight line(linear function) or quadratic function
    150 usd                           200 usd

    Supervised means that 'right answers' were given, and the goal
    is to produce more 'right answers'

    Regression problem - tring to predict continuous valued output (the price)

    Classification problem - tring to predict a discrete valued output (0 or 1)

*** Unsupervised learning 

    The data set has no 'right answers'/labels given,
    we have to find the structure into the data

    Task: Google News group stories
    Task: Genomics shared by individuals
    Task: Organize computer clusters
    Task: Social networks
    Task: Market Segmentation
    Task: Astronomical data analysis

    Clustering algorithm - tries to find clusters in data

    Problem: Cockteail party
    Person 1     Microphone 1
    Person 2     Microphone 2

    Task: separate voices of both persons

*** Model and Cost Function
    
**** Linear Regression with one variable

    Task: Housing prices

    Trainig Set ---> Learning Algorithm ---> h(x) ---> prediction of y

    h is function from x to y

    Cost function - we can use to measure the accuracy of h()
    or how to 'fit the best line into the data'

    This is like a fancy avarage function of all results of h()
    The difference between the predicted value and the actual value
    Also Squared error or Mean squared error function

    Hypothesis: h (x) = T0 + T1x

    Ti - parameters of the model

    Ex.:

    Hypothesis:      hT(x) = T0 T1x

    Parameters:      T0, T1

    Cost Function:   J(T0, T1) = 1/2m SUM m i=1 (hT(x^(i)) - y^(i))^2

    Goal:            minimize J(T0, T1)
                      T0, T1

*** Gradient descent

A general algorithm for minimizing the cost function J.
It is general and used all over the place in ml
