TODO: append here the full notes from Andrew Ng Machine Learning
Course, and move the python manual to separate file

* Machine Learning

* Week 1

** Intro

Arthur Samuel (1959): field of study that gives computers the
ability to learn without being exolicitly programmed.

in the '50 he wrote a checkers playing program that was able
by just playing 10's of 1000's of games to learn over time
and eventualy became much better than him.

Tom Mitchell (1998): a computer is said to learn from experience
E with respect to some task T and some performance measure P,
if its performance on T, as measured by P, improves with
experience E.

Definition:

Example with checkers:
E ::= the experience of playing many games
T ::= the task of playing the game
P ::= the probability of winning the next game

if P to do T increases as E increases it is machine learning


Machine learning algorithms:

Supervised learning
Unsupervised learning
Reinforced learning
Recommender systems

It is important to know the tools, but it is even more important
to know how to use them.

*** Supervised learning

Problem: Predict housing prices

y = price
x = size

So do you want to fit a linear or quadratic function, beacause
diff in price may be significant?

Supervised leraning means that we give the algorithm a data set
in which the "right answers" were given and it just needs to
produce more of them.

Regression problem means we are tring to predict a continuous
value output(the price x in this case).

Classification problem means we are tring to prodict a discreet
valued output.

In those kind of problems we need as many as features to prodict
on as possible. The Support Vector Machine algorithm helps with
a neat math trick that allows a computer to deal with even an
infinite number of features.

*** Unsupervised learning

Unsupervised learning is used where we don't have the labels
for the data and we are tring to find some structure in it.

Clustering algorithm will try to structure the data as clusters.

Problem: Google News topic clusters

One story leeds to others on the same topic.

Other problem domains:

Organise computing clusters
Social network analysis
Market segmentation
Astronomical data analysis

Problem: Cocktail party problem
Person 1     Microphone 1
Person 2     Microphone 2

Task: separate voices of both persons


Two microphones set at different distances(locations) in a
room with people similtaniously talking. Voice overlap in
a different way in each output(always mixed to a speech noice).
The algorithm will sum the outputs and we can clearly
distinguish each persons speech.

A difficult problem solved by researchers with just one line
of code:

[W, s, v] = svd((repmat(sum(x.*x,1), size(x,1),1).*x)*x');

** Model and Cost Function

m = number of training examples
x = "input" variable / features 
y = "output" variable / "target" variable

(x, y) = one training example(row in the table)  
(x^i, y^i) = i-th training example  

| Size (x) | Price (y) |
|----------+-----------|
| 2104     | 460       |
| 1412     | 232       |
| 1534     | 315       |
| ...      | ...       |

x^1 = 2014
y^1 = 460

Training set -> Learning Algorithm -> h() (Hypothesis)

Size h(Predicted price);
h :: Size x-> Predicted Price y

h maps from x to y

h_\theta (x) = \theta_0 + \theta_1 x

h(x) = a + bx

shorthand: h(x)
\theta 's are parameters of the model

Linear regression: where we use linear function as h(x)

Cost function: will help us fit the best h(x) to our data

In the housing price problem this is the best straight line that
fits the data. We need to minimize \theta_0 and \theta_1 so that
the difference between h(x) and y is small(or the difference
between the predicted value and the target in the data set is
small).

minimize J(\theta_0, \theta_1) = (1/2 * 1/m) * \sum_{i=1}^{m} (h(x) - y)^2

minimize the sum of the squared avarage error

The squared error cost function works well for regression.

[[./img/cost_function_full.jpg]]

** Gradient Discent

a very common general algorithm used to minimize functions.

Outline of the problem:
We have some function J(\theta_0, \theta_1)
We want to minnimaze it.

Start with some \theta_0, \theta_1
Keep changing \theta_0, \theta_1 to reduce J(\theta_0, \theta_1)
until we hopefully end up at a minimum
"We spin 360deg around us, choose the steepest way down and take a
small baby step into that direction, then repeat until you converge
into some minimum."


x = \theta_0
y = \theta_1
z = J(\theta_0, \theta_1)

repeat until convergence {
    \theta_j := \theta_j - \alpha d/d\theta_j J(\theta_0, \theta_1)
    (for j = 0 and j = 1)
}

temp0 := \theta_0 - \alpha d/d\theta_0 J(\theta_0, \theta_1)
temp1 := \theta_1 - \alpha d/d\theta_1 J(\theta_0, \theta_1)

\theta_0 := temp0
\theta_1 := temp1

Note: remember to always update the parameters \theta_0 and \theta_1
simultaniously(ater the calculation not during it);

\alpha - is a number known as the learning rate, which difeince the size of the
step that we take downhill.

d/d\theta_j J(\theta_0, \theta_1) - is the derivative term. When at an local
optimum slope is 0. And as we approach a local minimum the steps will
authomatically became smaller.

[[./img/gradient_discent.jpg]]

Gradient discent for Linear Regression

the cost function for the linear regression is going to be a bow shaped
function(a convex function). I t does not have any local optima, just the
global optima.

"batch" gradient discent - at every step we are looking at all the training
examples.

NOTE: Normal equations method is a numeric method to solve for the minimum
using linear algebra, but it does not scale as good as gradiaent discent.


* Week 2

** Multiple Features

multivariable linear regression:

h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n

h_\theta (x) = (transpose [\theta_0; \theta_1; ... \theta_n]) x [x_0; x_1; ... x_n] = \theta^T x

x_j^i = value of feature j in i-th training example (col)
x^i = i-th trainig example (row)
m = length of training examples (rows)
n = length of features (cols)

Think of i and j as the counters in a nested loop i is outer j is inner.
i loops the outer layer of training examples and j loops each example
training features.

Note: x_0 = 1 and \theta is n+1 dimentional vector (meaning that they are
passed to the function as one paramter - the vector)

gradient descent for multiple variables:

n >= 1

Repeat {
    \theta_j := \theta_j - \alpha 1/m \sigma_{i=1}^{m}(h_\theta(x^i) - y^i)x_j^i
    (simultaneously update \theta_j for j=0,...,n)
}

[[./img/gradient_discent_multiple_features.jpg]]


Feature scaling:
make sure features are on a similar scale
E.g.
x_1 = size(0-200 feet^2)
x_2 = number of bedrooms(1-5)

Will result in a very tall and skinny contour plot, which requires much
more work to be done by gradient discent.
So we better scale the features:

x_1 = size (feet^2) / 2000
x_2 = number of bedrooms / 5

This will result in less skewed and more circle like contours, which
are much more easy for gradient discent.

Get every feature into approximatly a -1 <= x_i <= 1 range

Mean normalization:
replace x_i with x_i - \mu_i to make features have approximately zero mean
(Do not apply to x_0 = 1). E.g.

x_1 = size - 1000 / 2000
x_2 = #bedrooms - 2 / 5

-0.5 <= x_1 <= 0.5
-0.5 <= x_2 <= 0.5

x_i = x_i - \mu_i / S_i

where \mu_i is the avarage value of x_i in the training set
and S_i is the range (max - min) or standart deviation.

Learning Rate (\alpha):

How to make sure that gradient discent is working correctly?
J(\theta) should decrease after every iteration.
Check the J, iterations graph.
It also possible to use automatic convergence test. Declare convergence if
J(\theta) decreases by less than 10^-3 in one iteration.

Possible solutions:
If J(\theta) is increasing or first decreasing a then increasing try using
smaller \alpha to avoid overshooting the minimum.
But if it is too small it will take many steps to converge.
To choose \alpha try: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1
try 3x bigger values until you choose the best largest one.

NOTE: When to stop the gradient descent iterations? Will it converge?
When the cost start to osciliate around a value appearing to very slowly
converge to it, we are close to a minimum. But it is also possible that
we've hit an inflection point, which by it self is not a minimum, but
may excibit a similar behavior.

NOTE: momentum is a way to optimeze the gradient descent algorithm and
make it pass over small 'hills' and not try to find a way around them.

Features and polynomial regression:

NOTE: it is even more important to scale your data when different features
may be of the form x_1^2 x_2^3 ...

** Computing Parameters Analytically

Normal equation:
for some linear regression problems will give us a much better way to
solve for optimal value of the parameters \theta. 


the X matrix and the y matrix:

[[./img/normal_equation.jpg]]

The formula for minimizing J(\theta):
\theta = (X^T X)^-1 X^T y

where X^T = X transposed and X^-1 = inverse of X

To compute in octave:

pinv(X' * X) * X' * y


NOTE: feature scaling is not needed in normal equation method

When to use what?

Gradient discent:
Works well even when n is large(number of features)
Need to choose \alpha
Needs many iterations

Normal equation:
No need to choose \alpha
Don't need to iterate
Need to compute (X^T X)^-1 and n x n invertion is O(n^3)
Slow if n is very large (n = 10000 may be too much)

Noninvertability:

What X^T X is non-invertable?

pinv - compute seudo-inverse
inv - compute inverse

May be because of:
Redundant features(linearly dependent).
E.g.
x_1 = size in feet^2
x_2 = size in m^2

Too many features
m <= n

* Week 3

** Classification and Representation

*** Classification

A class of problesms where the output value that we want to predict is a
descrete value.

For example:
Email: spam/not spam?
Online Transaction: fradulent(yes/no)?

y \in {0,1}

0: "Negative Class"
1: "Positive Class"

if y \in {0,1,2,3,4} this is called multi class problem

An approach to solve a classification problem with linear regression is
to run a threshold line from y=0.5 to the point where it crosses the
hypothesis line (0 <= y <= 1). and if h(x) >= 0.5 predict y=1 else if
h(x) < 0.5 predict y=0.
The problem here is that examples with high x values (positioned very
far right on the x-axis) will drag the hypothesis line towards them.
Ultimately linear regression wont work, just because classification
is not a linear problem.

*** Logistic Regression Model

Instead we want to use Logistic Regression, which despite of the name
is one of the most popular and used classification algoritms.

in lineear regression the model was:
h_\theta(x) = \theta^T x

for logistic regression we want only to midify it:
h_\theta(x) = g(\theta^T x)

where

g(z) = 1 / 1 + e^-z

which is known as a sigmoid function(logistic funciton).
If we put does 2 equations together we get: 

h_\theta(x) = 1 / 1 + e^{-\theta^T x}

The sigmoid funciton crosses y at 0.5 and has asimptotes at y=0 and y=1.

We interpret the output of h\theta(x) as the estimated probability that
y equals 1 on the input x.

h\theta(x) = P(y=2|x;\theta)
reas as: probability that y=1, given x, parameterized by \theta.

P(y=0|x;\theta) + P(y=1|x;\theta) = 1

P(y=0|x;\theta) = 1 - P(y=1|x;\theta)

*** Decision Boundary

The decision boundry is a property of the hypotesis and the parameters
of the hypothesis and not a property of the dataset.

Predict that y=1 if \theta^T x >= 0

h\theta(x)=g(\theta\_0 + \theta\_1 x\_1 + \theta\_2 x\_2)

-3 + x_1 + x_2 >= 0
x_1 + x_2 >= 3

x_1 + x_2 = 3 corresponds to the h\theta(x) = 0.5

[[./img/decision_boundary.jpg]]

Non-linear decision boundary:

if we add polynomial terms to the hypothesis we can get a more
complex funciton to fit on our classifier.

x_1^2 + x_2^2 = 1 is the equation for a circle with radius 1, and it can
fit y = 1 outside the circle and y = 0 inside the circle.

[[./img/nonlinear_decision_boundary.jpg]]

*** Cost Function

We have a training set of m examples and it is represented with a feature
vector that is R^{n+1} and has the zero feature x_0 = 1, and because it
is a classification problem our training set has the property that y is
0 or 1.

How do we fit the parameters \theta?

[[./img/logistic_reg_cost_problem.jpg]]

We need to rewrite the cost function.

Cost(h_\theta(x), y) = 1/(h_\theta(x) - y)^2

But because of the sigmoid function in logistic regression our cost is
now a non convex function and those are hard to optimize.

When the algorithm predicts h(x) is y it is going to pay a cost:

-log(h(x))       if y was 1 and
-log(1 - h(x))   if y was 0

[[./img/log_cost_function.jpg]]

Cost = 0 if y=1, h(x)=1
But as h(x) \to 0 Cost \to \inf


[[./img/log_cost_function_y0.jpg]]

Turns out there is a way to simplify our function and compress
the 2 conditional cases into one:

[[./img/simplified_cost_function.jpg]]


Gradient descent for logistic regression:

[[./img/gradient_descent_logistic_regression.jpg]]


*** Advaced Optimization

Optimization algorithms:
- Gradient Descent
- Conjugate gradient
- BFGS
- L-BFGS

The last 3 use more and more sophisticated strategies to minimaze the
cost function. They have more clever inner loop called line search
algorithm that tries out automatically different values for the
learning rate alpha and picks to ultimate one. They also converge
much faster through other optimizations.

Advatages:
- no need ti manually pick \alpha
- ofden faster than gradient descent
Disadvantages:
- more complex

[[./img/advanced_optimization.jpg]]

for linear regression:

[[./img/advanced_optimization_linear_regression.jpg]]

for logistic regression:

[[./img/advanced_optimization_logistic_regression.jpg]]


** Multiclass Classification

Email foldering/tagging: Work, Friends, Family, Hobby

y = [ 1: "Work"; 2: "Friends"; 3: "Family"; 4: "Hobby"];

One vs All(Rest) classification method turns multiclass a problem into
multiple binary classifications.
Train a logistic regression classifier h_\theta^i(x) for each class i
to predict the probability that y = i. max_i h_\theta^i(x).

h_\theta^1(x)

h_\theta^2(x)

h_\theta^3(x)

[[./img/onevsall_multiclass_classifier.jpg]]



** Overfitting

*** The Problem of Overfitting

Underfitting is when the  algorithm has "high bias", stong preconceptions
about the data. It is trying to fit a line to a more complex set.

Overfitting is when the algorithm has "high variance", the space of
possible hypothesis is too large and we don't have enough data to constrain
it to give us a good hypothesis.
In other words if we have too many features, the learned hypothesis may
fit the training set very well J(\theta) ~ 0, but fail to generalize to
new examples.

To avoid:
try to reduce the number of features
regularization - keep all the features, but reduce magnitude/values of
parameters \theta_j

*** Regularization

When we have too many terms in our hypothesis function it may start to
overfit. To solve this we can try to reduce the weight of some of those
terms by increasing the their cost.

For example by making a n^4 (high order polynomial) function more like
n^2 (quadratic):

\theta_0 + \theta_1 * x_1 + \theta_2 * x_2^2 + \theta_3 * x_3*3 + \theta_4 *
x_4^4

a + b^2 + c^3 + d^4

Again we don't want to eliminate these features because they are important
for our model, but we can increase their cost:

(\sum_{i=1}^{m} (a + b^2)) + 1000*c^2 + 1000*d^2

if we don't know which parameters to shrink we can use the \lambda
and summation over all of them:

\lambda \sum_{i=1}^{n} \theta\_j\^2

if we set the value of \lambda too high it will
if we set it to 0 this will eliminate the effect of the parameters and
make them equal to 0, which is underfitting (maybe a straight line or
something).


[[./img/hypothesis_regularization_of_terms_cost_1.jpg]]





Gradient Descent

[[./img/regularization_linear_regression_gd.jpg]]


Normal Equation

[[./img/regularization_linerar_regression_normal_equation.jpg]]




** OCTAVE Quickstart guide

1 == 2 % false
1 ~= 2 % not
a = 3
a = 3; % semicolon supressing output
b = 'hi';

a = pi;
a

disp(a)
disp(sprintf('2 decimals: %0.2f', a)); % like c formating

format long a

A = [1 2; 3 4; 5 6]
v = [1 2 3];
w = ones(1,3); % 0 0 0
w = zeros(1,3); % 1 1 1
w = rand(1,3);
w = randn(1,3);
% drawn from gaussian distribution with mean 0 and variance
% or standart deviation equal to 1

plot(w)


eye(4) % identity matrix 4 x 4

help eye % to see help for eye command

A = [1 2; 3 4; 5 6]
size(A); % ans = 3 2
sz = size(A);
size(sz); % 1 2

size(A, 1); % size of 1st dimention

v = [1 2 3 4];
length(v); % size of the longest dimention

load featuresX.dat
load priceY.dat
load('featuresX.dat')

who % prints variables in memory
featuresX % will display the features from the file
whos % will list with more info like size and class

clear featuresX % to remove variables from memory

save hello.mat v; % will save the value of v from memory to the file hello.mat

clear % will clear all memory

A(3,2); % ans = 6
A(2,:); % everiting in the second row
A(:,2); % everithing in the second col
A([1 3], :); % get rows 1 and 3 and give all cols from them
A([1 3], 1); % but give only 1st col

A(:,2) = [10; 11; 12]; % will assaign to the second col of A
A = [A, [100; 101; 102]]; % will append another col
C = [A B]; % will concatenate A and B into C, with B next to A
C = [A; B]; % will concatenate them with A on top of B

A * C % matrix multiplication
A .* B % element wise multiplication
A .^ 2 % element wise squaring of A
1 ./ v % element wise reciprocal
1 ./ A
log(v) % element wise log
exp(v) % exponentiation of each element
abs(v) % element wise absolute value
-v % the same as -1 * v

v + ones(length(v), 1); % add 1 to each elememt
v + 1
v .+ 1

A' % transpose A

a = [1 15 2 0.5];

max(a); % ans = 15

[value, index] = max(a); % will assaign the value and the index of the max

a < 3 % element wise comparison ans = 1 0 1 1
find(a < 3); % will return the positions of the < 3 elements

A = magic(3); % all rows and cols sum up to the same number

sum(A); % sum all the col elements of A
prod(A); % product of all the col elements in A
sum(sum(A)) % to get sum of the cols twice
sum(A, 1); % sum every col in A
sum(A, 2); % sum every row in A

A .* eye(3) % will return just the diagonal elements(else fill with 0s)
sum(sum(A .* eye(3))); % to sum the diagonal
sum(sum(A .* flipud(eye(3)))); % to sum up the left to right diagonal

pinv(A); % inverts a matrix

floor(a);
ceil(a);

rand(3); % 3 x 3 random matrix

max(A, [], 1); % take the col wise max value
max(A, [], 2); % take the row wise max value

A(:); % will turn A into a vector
max(max(A));
max(A(:));

Plotting:

t = [0:0.01:0.98]; % create range from 0 to 0.98 with step 0.01
y1 = sin(2*pi*4*t);
y2 = cos(2*pi*4*t);
plot(t, y1);
plot(t, y2);

hold on;

plot(t, y2, 'r');
xlabel('time');
ylabel('value');
legend('sin', 'cos');
title('plot of sin and cos');

print -dpng 'sin_cos_plt.png'
close
figure(1); plot(t, y1);
figure(2); plot(t, y2);

subplot(1,2,1); % divides plot a 1x2 grid, access first element
axis([0.5 1 -1 1]);
% x axis will range from 0.5 to 1, while y axis from -1 to 1
clf % clears a figure

imagesc(A); % will plot a colormap of the matrix A
imagesc(A), colorbar, colormap gray; % , is used to chain commands

Functions and Control Statements:

for i=1:4,
    v(i) = 2^i;
end;

i = 1;
while true,
    v(i) = 999;
    i = i + 1;
    if i == 6,
        break;
    end;
end;

if i == 1,
    disp('1');
elseif i == 2,
    disp('2');
else
    disp('not 1 or 2');
end;

funciton squareThisNumber(x)
    x * x
end

Vectorization:

representing comutation with linear algebra instead of loops may speed
up calculations, but may also take much more memory.



* Other

** Octave Reference

** Python Reference

*** Conditionals

if cond1:
    do()
ifel cond2:
    do()
else:
    do()

*** Loops:

>>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
>>> numbers = [1, 2, 3, 4, 5, 6, 7, 8]

>>> for k, v in knights.items(): print(k, v)

>>> for x in numbers: print(x)

>>> for i in range(10): do()

*** Collections:

   Seqs are mutable(list) and immutable(tuple, range)

**** tuple

>>> t = (1,2,3,4)

    immutable,
    collection
    - packing and unpacking

>>> t = 12345, 54321, 'hello!'
>>> x, y, z = t


**** list

>>> l = [1,2,3,4]

    mutable,
    more like array or vector or linked-list?
    - methods:
      .append(x)                - adds at the end
      .extend(iterable)
      .insert(i, x)
      .remove(x)                - remove first by value
      .pop([i])                 - if no arg, returns last
      .clear()
      .index(x[,start[,end]])   - return 0-based index of value
      .count
      .sort
      .reverse
      .copy
      del a[i:j]                - removes by index at pos or range

    - as stacks:
      with .append() and .pop()

    - as queues:
      with .append() and .pop(0)

    - list comprehensions:

>>> [x**2 for x in range(10)]
>>> [x for x in vec if x >= 0]
>>> [(x, x**2) for x in range(6)]

>>> # flatten a list using a listcomp with two 'for'

>>> vec = [[1,2,3], [4,5,6], [7,8,9]]
>>> [num for elem in vec for num in elem]

Out[]: [1, 2, 3, 4, 5, 6, 7, 8, 9]

    - queues: better use collections deque

>>> from collections import deque
>>> queue = deque(["Eric", "John", "Michael"])
>>> queue.append("Terry")           # Terry arrives
>>> queue.append("Graham")          # Graham arrives
>>> queue.popleft()

    - map:

>>> squares = list(map(lambda x: x**2, range(10)))

    - reduce:
>>>

    - filter:
>>>

**** dict

>>> tel = {'jack': 4098, 'sape': 4139}

    classical hash map

    - methods:
    d[key]                 - to access by key
    del d[key]             - remove
    .iter(d)               - return an iterator over the keys of the dictionary
    .clear()
    .copy()
    .get(key[,default])    - get value else default value
    .keys()
    .values()
    .items()
    .pop(key[,default])
    .popitem()             - remove arbitrary
    .update([other])       - overwrites existing

>>> dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])
>>> dict(sape=4139, guido=4127, jack=4098)

**** set

>>> basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}

    like math sets

>>> 'orange' in basket
Out[]: True

>>> a = set('abracadabra')
Out[]: {'a', 'r', 'b', 'c', 'd'} # unique letters in a


**** range

>>> range(1,10)

    range(start, stop, step)

    it is useful, because it is more efficient. Values are generated only when needed
    lazy seq

**** Common Seq Operations

    x in s 	              True if an item of s is equal to x, else False
    x not in s 	          False if an item of s is equal to x, else True
    s + t 	              the concatenation of s and t
    s * n or n * s 	      equivalent to adding s to itself n times
    s[i] 	                ith item of s, origin 0
    s[i:j] 	              slice of s from i to j
    s[i:j:k] 	            slice of s from i to j with step k
    len(s) 	              length of s
    min(s) 	              smallest item of s
    max(s) 	              largest item of s
    s.index(x[, i[, j]]) 	index of the first occurrence of x in s (at or after index i and before index j)
    s.count(x)

** Functions:

- argument lists
- keyword arguments
- default arguments

- destructuring

a,b,c = abc(): return a, b, c


- recur
   no tco!?

** Decorators
** OOP
** Generators

*** Unsupervised learning

    The data set has no 'right answers'/labels given,
    we have to find the structure into the data

    Task: Google News group stories
    Task: Genomics shared by individuals
    Task: Organize computer clusters
    Task: Social networks
    Task: Market Segmentation
    Task: Astronomical data analysis

** Deep learning for NLP

    Traditional machine learning uses more 'hard-coded' methods and requires
    experienced and deeply knowlegable domain specialists to code them.
    Deep learning uses vectors as more efficient and simpler abstraction
    in order to turn tml on its head.

    Representations of language

    |           |      TML         |    DL   |
    +-----------+------------------+---------+
    | phonology | all phonemes     | vectors |
    | morphology| all morphemes    | vectors |
    | words     | one-hot encoding | vectors |
    | syntax    | phrase rules     | vectors |
    | semantics | lambda calculus  | vectors |

    NOTE: one-hot encoding (uses matrix, not very efficient)

    |     | The | cat | sat | on | the | mat |
    | The |  1  |  0  |  0  |  0 |  1  |  0  |
    | cat |  0  |  1  |  0  |  0 |  0  |  0  |
    ...

    Applications

    Easy: spell checking, synonym suggestions, keyword search
    easy to bruteforce with tml

    Intermediate:
    reading level,
    extracting information,
    predicting next word,
    classification

    Complex:
    machine translation,
    quation answering,
    chatbots,

** RNN (Recurrent Neural Networks)

Vanila NNs and Convolutional NNs have constrained API
They operate over fixed:
input vectors, output vectors and computational steps

RNNs allow to operate over sequances of vectors

RNNs might be just unreasonably effective,
despite them being considered hard to train

Input Sequence Vectors <-> State Vectors -> Output Sequence Vectors


They are Turing-Complete
If training vanilla neural nets is optimization over functions,
training recurrent nets is optimization over programs
