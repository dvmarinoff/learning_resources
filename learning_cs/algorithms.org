** Sorting (Insertion Sort, Merge Sort)

*** Why?

Many problems like searching become easy when items are sorted.

- Finding a medien for example:

unsorted array xs[0:n] -> sorted array B[0:n]

B[n/2] = medien

- Binary Search:

if xs[0:n] is sorted we are lookig for specific item k in log time

The simplest paradigm for Divide and Conquer

- Data compression:

sort items and automatically find the duplicates to eliminate

- Computer Graphics

You want to render front to back, so you need items sorted
Sometimes (to account for transperancy) you need it sorted back to front

*** Insertion Sort

In place sorting algorithm, in \Theta (n log n)

For i = 1, 2, \dots, n
insert xs[i] into sorted array xs[0:i-1]
by pairwise swaps down to the correct possition

[5,2,4,6,1,3] -> [2,5,4,6,1,3] -> [2,4,5,6,1,3] -> [2,4,5,6,1,3]
-> [1,2,4,5,6,3] -> [1,2,3,4,5,6]

5 is first so it is sorted, we start from 2 where is the key.
The key moves left to rigth and swaps.

\Theta (n) steps (key positions)
Each step is \Theta (n) compares and swaps
But if you are comparing entities and not numbers, the swap function migth
be more complex and require more steps.

In case Compares are more expensive than Swaps we have:
\Theta (n^2) compares cost

If we replace pairwise swaps with binary search we get \Theta (n log n)

*** Merge Sort

Divide and Conquer, recursive algorithm

xs -> split to -> L and R -> sort -> L' and R' -> merge -> to sorted xs'

[[./img/6006/merge_sort.jpg]]

size n -> 2 unsorted size n/2 -> 2 sorted size n/2 -> size n


Merge: assumes two sorted arrays as input

L' = [20,13,7,2] and R' = [12,11,9,1]

Think of it as a two finger algorithm:
Start at one end and compare 2 elements from L' and R'
Move the smallest to the sorted array. Repeat.

#+BEGIN_SRC
>> compare 1 and 2
>> move 1 to sorted array
>> compare 2 and 9
>> move 2 to sorted array
>> compare 7 and 9
>> move 7 to sorted array
>> \dots
#+END_SRC

[[./img/6006/merge_routine.jpg]]

The merge routine does most of the job and is \Theta (n)
The overall complexity of the algorithm is \Theta (n log n)

Thm.: Merge sort is \Theta (n log n)

Proof: 

By looking at the merge routine we can construct a recurrence that looks
like this.
Complexity: T(n) = C_{1} + 2T(n/2) + C * n,

where C_{1} is the divide step, 2T(n/2) is the recursive part, and C*n is
the merge part 

C*n -> C*n/2 C*n/2 -> C*n/4 C*n/4 C*n/4 C*n/4 -> \dots -> C \dots C

the number of levels is 1+log(n), the number of leaves is n
and you're doing the same amount of work C*n at each level:

T(n) = C*n * (1 + log(n)) = \Theta (n * log n)

[[./img/6006/merge_sort_complexity.jpg]]

Merge sort needs \Theta (n) auxilary space for (L,R) and (L',R'),
Insertion sort (in place sort) needs \Theta (1) auxilary space
Consider this if you need to sort like billion of items.
There is an advanced in place merge sort algorithm, but is inpractical
with its worse running time.

In python:

Merge sort is: 2.2 * n * log(n) ms
Insertion sort is: 0.2 * n^{2} ms 

In C:
Insertion Sort is: 0.01 * n^{2} ms  


*** Recurrencies


T(n) = 2T(n/2) + C*n^{2}

C*n^{2} -> C*n^{2}/4 + C*n^{2}/4 -> C*n^{2}/16 + C*n^{2}/16 + C*n^{2}/16 + C*n^{2}/16

1+log(n) levels, n leaves,
#+BEGIN_SRC
work at leval 1: C*n^{2}
work at leval 2: C*n^{2}/2
work at leval 3: C*n^{2}/4
\dots
work at leval 1+log(n): C*n
#+END_SRC

top level C*n^{2} dominates, complexity is \Theta (n^{2})

T(n) = 2T(n/2) + \Theta (1) or C

C -> C + C -> C + C + C + C -> \dots -> n * C
#+BEGIN_SRC
work at leval 1: C
work at leval 2: 2C
work at leval 3: 4C
\dots
work at leval 1+log(n): nC
#+END_SRC

the bottom level n * C dominates, complexity is \Theta (n)

** Heaps and Heap Sort

Heap as example implementaion of priority queue, and base for sorting algorithm
known as Heap Sort.

*** Priority Queue

Implements a set S of elements, each of elements associated with a key.
Operations: pick max or min, delete, insert, change

insert(S, x)   : insert element x into set S

max(S)         : return element of S with a largest key

extract_max(S) : as max(s), but also removes it from S

increase_key(S,x,k) : increase the value of x's key to k

*** Heap
 
xsn array visualized as a nearly complete binary tree.

[16, 14, 10, 8, 7, 9, 3, 2, 4, 1]

10 elements is not 15, so it is not a full binary tree
index 1 is the root of the tree, 2 and 3 are the children of 1

- Heap as a tree:

root of  tree: 1st element (i=1)

parent(i) = i/2

get_left(i) = 2i, right(i) = 2i + 1

xs Heap must keep the Heap property correct:
- Max-Heap property: the key of a node is >= the key of its children
- Min-Heap property: the key of a node is <= the key of its children

Heap operations:

build_max_heap : produces the max heap from an unordered array

max_heapify(xs, i) : correct a single violation of the heap property in
                    a subtree' s root


Given the asumption that the trees rooted at get_left(i) and right(i) are
max-heaps the complexity of the max_heapify is bound by the heigth of
the tree and the tree is nearly balanced binary tree => \Theta (\log n)

**** max_heapify pseudocode:

#+BEGIN_SRC ruby

def max_heapify (xs, i)
  left = get_left(i)
  right = get_right(i)

  if left <= heap_size(xs) and xs[left] > xs[i]
    largest = left
  end

  if right <= heap_size(xs) and xs[right] > xs[largest]
    largest = right
  end

  unless largest == i
    exchange(xs[i], xs[largest])
    max_heapify(xs, largest)
  end
end
#+END_SRC

#+BEGIN_SRC ruby

# build_max_heap :: [Int] -> [Int]
def build_max_heap(xs)
  middle = xs.size / 2
  
  middle.downto(1) do |i|
    max_heapify(xs, i)
  end
end

#+END_SRC

**** Analysis:

Total amount of work in the loop of max_heapify is the sum:

n/4(1 c) + n/8(2 c) + n/16(3 c) + \dots + 1(log(n) c)

Set n/4 = 2^{k} and simplify:

c 2^{k}( 1/2^{0} + 2/2^{1} + 3/2^{2} + \dots + (k + 1)/2^{k} )

bounded by a constant -> build_max_heap() is \Theta (n)

*** Min-Heap

[[./img/6006/min_heap.jpg]]

*** Heap Sort

turn unordered array into a max-heap and do extract max

Sorting strategy:
1. build Max Heap from unordered array
2. find maximum element A[1]
3. swap elements A[n] and A[1], now max element is at the end of the array
4. discard node n from heap (by decrementing heap size number)
5. new root may violate max heap property, but its children are max heaps.
Run max_heapify to fix this
6. go to step 2 unless heap is empty

** Binary Search Trees, BST Sort 

binary search is fundamental divide and conquer paradigm and there is a DS
associated with it called binary search tree.

Illustrated with a toy problem about runway scheduling.

**** Runway reservation system:
1. single runway airport
2. build for reservations for future landings
3. we want to reserve a request for landing with landing time t
4. Add t to the set of R if no other landings are scheduled within k minutes

We want the operations in: O (log n) time, where R = n

Example:

[[./img/6006/runway_scheduling_example.jpg]]

The problem: no convenient DS for the insert and search OPS

| DS                  | insert    | check     | append          | sort            |
|---------------------+-----------+-----------+-----------------+-----------------|
| Sorted list         | O(n)      | O(1)      | \Theta(n log n) | \Theta(n log n) |
| Sorted array        | \Theta(n) | O(log n)  |                 |                 |
| Unsorted list/array |           | O(n)      |                 |                 |
| Min-Heap            | O(log n)  | O(n)      |                 |                 |
| Dict/Set            | O(1)      | \Omega(n) |                 |                 |
| BST                 | O(h)      | O(log n)  |                 |                 |

**** Binary Search Tree:

unlike the heap which is just an array and we have to visualize it being a
tree, the BST is an actual tree that has pointers - parent(x), left(x) and
right(x).

What makes a tree a BST is the property that it satisfies the invariant:

#+BEGIN_SRC
for any node x, for all nodes y in the left subtree of x, key(y) <= key(x)
for all nodes y, in the right subtree of x key(y) >= key(x)
#+END_SRC

[[./img/6006/binary_search_tree_insert.jpg]]

But do we have a solution?

insertion with check is O (h) time, where h is the height of the tree
and h = n is possible

Other operations:

find_min() is O (h)
next_layer(x) is O (h)

**** Augmented Binary Tree

Client adds new requirement:
Be able to compute Rank(t) = how many planes are scheduled to land at times <=t

What lands before t?
1. Walk down the tree to find desired time
2. Add in the nodes that are smaller
3. Add in the subtrees size to the left


** AVL Trees, AVL Sort

BSTs support insert, delete, min, max, next_larger, etc in O(h) time, where
h = length of the longest path from root to leaf (down).
In the worst case for BSTs h = n, and they are more like a list.
A tree is balanced if h = O(log n)

height of a node = length (# edges) of longest downward path to a leaf

*** AVL trees (Adel'son-Vel'skii & Landis 1962)

Using the formula we can store the heigths of the nodes for free:
max(height(left_child), height(right_child)) + 1

Our goal is to keep the heights small.

nil childs are -1, so by the formula: max(-1,-1) + 1 = 0

For every node, require heights of left and right children to differ by at
most +-1 each node stores its height like subtree size or just diffrence 
in heights.

wosrt case is when the right subtree has height 1 more than the left for
every node.

N_{h} = min # nodes in an AVL

N_{0(1)} = O(1)

N_{h} = 1 + N_{h-1} + N_{h-2}, <=
(The root = 1) + (right subtree = h - 1) + (left subtree = h - 2)

looks a lot like fibonachi, just remove the 1

=> N_{h} > F_{h} = \varphi^{h}/sqrt{5}

> 1 + 2 * N_{h-2} > 2 * N_{h-2} = \Theta(2^{h/2})

h < 2 \log_{2}(n) => \Theta(log n) 

How do we maintain the property?

AVL insert:
1. Simple BST insert
2. work your way up tree, restoring AVL property(and updating heights)

Rotation: 

Left rotate:

[[./img/6006/avl_left_rotate.jpg]]

Right rotate:

[[./img/6006/avl_right_rotate.jpg]]


*** AVL Sort

very powerful way to sort in O(n log n)

- insert n items - \Theta(n log n)
- in-order traversal - \Theta(n)

** Counting Sort, Radix Sort, Lower Bounds for Sorting and Searching

We start with a Thm, a proof and a counter example. We argue that the lower
bound for searching is \Omega(\log(n)) and for sorting is \Omega(n \log n),
but we see that we can get away with just n for some special cases. 

*** Claims:

1. searching among n preprocessed items requires \Omega (log n) time:
binary search, AVL tree search are optimal

2. sorting n items requires \Omega(n log n):
mergesort, heap sort, AVL sort are optimal

*** Comparison Model

A new model of computation useful for proving lower bounds.

- Lets restrict the kind of operations to be only comparisons:
( <, <=, >, >=, = ), only binary answer yes or no. 
 
- All input items are black boxes, we don't know what they arei(ADTs).

- Time cost is only the # comparisons

**** Decision Tree

If know that our algorithm is only comparing items we can draw all the
possible things that an algorithm can do.
Any comparison algorithm can be viewed/specified as a tree of all possible
comparison outcomes and resulting output, for a particular n.

Example: binary search for n = 3

1. internal nodes = binary decisions
2. leafs = algorithms is done(output)
3. root-to-leaf path = algorithm execution
4. path length (depth) = running time
5. height of tree = worst-case running time

Binary decision tree is more powerful than comparison model,
and lower bounds extend to it

[[./img/6006/decision_tree.jpg]]

*** Lower Bounds
    
**** Searching lower bounds

Thm:
Given n preprocessed items, meaning we get sorting or structuring in a heap
for free, to find a given item among them in comparison model requires
\Omega(\log(n)) in worst case.

Pf:
Decision tree is binary and it must have >=n leaves, one for each answer.
-> height >= \log(n)

1. # leaves >= # possible answers >= n
2. decision tree is binary
3. -> height >= log \Theta(n) = log n \pm \Theta(1)

**** Sorting lower bounds

A[i] < A[j] -> Yes or No

Decision tree is binary and # leaves is atleast the # of possible answers
(answer may appear in several leaves) = n!

-> height is >= \log(n!)

Use Sterling formula or Sum to proove that it is n \log(n)

1. leaf specifies answer as permutation: A[3] <= A[1] <= A[9] <= \dots 
2. all n! are possible answers
3. # leaves >= n!
   
4. in fact \log(n!) = n \log(n) - O(n) via Sterlings formula:

-> height >= \log(n!)
= \log(1 * 2 * \dots * (n - 1) * n)
= \sum_{i=1}^{n} \log(i)
>= \sum_{i=1}^{n/2} \log(i)
>= \sum_{i=1}^{n/2} \log(n/2)
= n/2 * \log(n) - n/2
= \Omega(n \log n)

n! ~ \sqrt{2 \pi n}(n/e)^{n} -> \log(n!) ~ n \log(n) - O(n)

*** Linear-Time Sorting (special case for sorting Integers)

Lets use the RAM model here. We can do more than only comparisons.

\O(n \sqrt{\log \log n}) is the best algorithm here, but when n is small
it is possible to do it in linear time.

**** Counting Sort:

[3,5,7,5,5,3,6]

1. Count the items     

[3, 3, 5, 5, 5, 6, 7]

2. \dots

Those kind of algorithms sort only Integers, but those Integers can carry
other stuff with them.

#+BEGIN_SRC 
L = array of k empty lists

for j in n
  L[key(A[j])].append(A[j])

output = []

for i in k
  output.extend(L[i])

#+END_SRC

**** Radix Sort: 

For sorting numbers. Sorts the numbers from least to most significant digit.

we need n buckets and k passes over the numbers,
where k is the # digits of the bigest number.

xs = [10, 15, 1, 60, 5, 100, 25, 60]

1. find largest number -> 100
2. count # digits in largest number -> 3
3. right pad the rest of the numbers ->

[010, 015, 001, 060, 005, 100, 025, 060]

4. pass 1: sort by using the 1st digit only ->

[010,060,100,050], [001], [], [],[],[015,005,025], [], [], [], []

take numbers out of buckets strating from bottom (1st number in array) ->

[010,060,100,050,001,015,005,025]

5. pass 2: sort by using the 2st digit only ->

[100,001,005], [010,015], [025], [], [],[050], [060], [], [], [] ->

[100,001,005,010,015,025,050,060]

6. pass 3: sort by using the 3st digit only ->

[001,005,010,015,025,050,060], [100], [], [], [], [], [], [], [], [] ->

[001,005,010,015,025,050,060,100]

7. remove the leading zeros

** Haching I: Chaining

*** Dictionary

ADT maintaining a set of items, each with a key. Perhaps the most popular
data structure is CS.

- build into most languages
- databases use tree search or hashing
- compilers and interpreters(mostly old ones)
- network routers, network server, virtual memory
- substring search
- string commonalities
- cryptography

OPS:

- insert(item) - add item to set
- delete(item) - remove item from set
- search(key)  - return item with key if it exists

We assume items have distinct keys.

Balanced BSTs solve in O(\log n) time per op.
Our goal is O(1) per operation.

**** Direct Access Table

Items stored in an array indexed by keys(random access)

Problems:
1. keys must be nonnegative Integers (or using two arrays, Integers),
it is hard to associate something with an Integer
2. large key range -> large space - e.g. one key of 2^{256} is not good,
it is gigantic memory hog

Solutions:

Solution to 1 is prehashing:

maps keys to nonnegative Integers

- In theory, possible because keys are finite -> set of keys is countable
- In Python: hash(object), where object is a number, string, tuple, etc. or
object implementing \__hash__, maps the thing to an Integer
- In theory, x = y -> hash(x) = hash(y)
- Python applies some heuristics for practicality: for example,
hash('\0B') = 64 = hash('\0\0C')
- Object's key should not change while in table (else cannot find it anymore)
- No mutable objects like lists 

Solutions to 2 is hashing:

We have a giant key space \U and if we store it in the direct access table,
this will also be giant, so we want to map it using a hash function h down
to some smaller set m the size of our hash table. Then there is a subset
of \U with keys that are actualy stored in the dictionary. That set changes.
The idea is that we would like m to be around n, m = \Theta(n), so will use
linear space O(n).

The problem is that in this case space m will be too small and there might
be collisions.

- Reduce universe \U of all keys(say, Integers) down to reasonable size m for table
- idea: m ~ n = # keys stored in dictionary
- hash function h: \U -> { 0, 1,..., m - 1 }
- two keys k_{i}, k_{j} \in K collide if h(k_{i}) = h(k_{j})

How do we deal with collisions?
1. Chaining
2. Open addressing

*** Chaining   

If you have colliding items, just store them in a linked list.

- The search must go through the whole linked list T[h(key)]
- Worst case: all n keys hash to same slot -> \Theta(n) per op
In the case of hashing randomization helps to keep you well away from the
worst case analysis.

[[./img/6006/hashing_with_chaining.jpg]]  

Expected cost of insert/delete/search is O(1 + \alpha)

*** Simple Uniform Hashing

An assumption (cheating): each key is equally likely to be hashed to any
slot of table, independant of where other keys are hashed.

let n = # keys stored in table,
let m = # slots in table

load factor \alpha = n/m = expected # keys per slot = expected length of a chain

Performance:

This implies that running time for search is \Theta(1 + \alpha), where the 1
comes from appling the hash function and random access to the slot whereas
the \alpha comes from searching the list. This is equal to O(1) if
\alpha = O(1), i.e., m = \Omega(n)

*** Hash Functions

We cover 3 methods to achieve the above performance:

**** Division Method:

h(k) = k mod m, gives you a umber between 0 and m-1

In most situation it is a bad choice.
It can be practical when m is prime, but not too close to power of 2 or 10.
But it is inconvenient to find a prime number, and division is slow.

**** Multiplication Method:

h(k) = [(a * k) mod 2^{w}] >> (w - r)

, where >> means shift right
, w is w bit machine from models of computations

where a is random, k is w bits, and m = 2^{r}. This is practical when a is
odd and 2^{w-1} < a < 2^{w} and a is not too close to 2^{w-1} or 2^{w}.

Multiplication and bit extraction are faster than division.
But in theory this method is also bad.

**** Universal Hashing

Now for the cool one.

For example:

h(k) = [(ak + b) mod p] mod m,

where a and b are random \in {0,1,\dots,p-1}, and p is a large prime (> |\U|)  

This implies that for worst case keys k_{1} != k_{2}
the probability of 2 them colliding is 1/m:

Pr_{a,b}{event X_{k_{1}k_{2}}} = Pr_{a,b}{h(k_{1}) = h(k_{2})} = 1/m

This implies that:

E_{a,b}[# collisions with k_{1}] = E[\sum_{k_{2}} X_{k_{1}k_{2}}]

= \sum_{k_{2}} E[X_{k_{1}k_{2}}]

= \sum_{k_{2}} Pr{ X_{k_{1}k_{2}} = 1}

= n/m = \alpha

** Hashing II: Table Doubling, Karp-Rabin
