* 6.006 - Introduction to Algorithms
** Sorting (Insertion Sort, Merge Sort)

*** Why?

Many problems like searching become easy when items are sorted.

- Finding a medien for example:

unsorted array xs[0:n] -> sorted array B[0:n]

B[n/2] = medien

- Binary Search:

if xs[0:n] is sorted we are lookig for specific item k in log time

The simplest paradigm for Divide and Conquer

- Data compression:

sort items and automatically find the duplicates to eliminate

- Computer Graphics

You want to render front to back, so you need items sorted
Sometimes (to account for transperancy) you need it sorted back to front

*** Insertion Sort

In place sorting algorithm, in \Theta (n log n)

For i = 1, 2, \dots, n
insert xs[i] into sorted array xs[0:i-1]
by pairwise swaps down to the correct possition

[5,2,4,6,1,3] -> [2,5,4,6,1,3] -> [2,4,5,6,1,3] -> [2,4,5,6,1,3]
-> [1,2,4,5,6,3] -> [1,2,3,4,5,6]

5 is first so it is sorted, we start from 2 where is the key.
The key moves left to rigth and swaps.

\Theta (n) steps (key positions)
Each step is \Theta (n) compares and swaps
But if you are comparing entities and not numbers, the swap function migth
be more complex and require more steps.

In case Compares are more expensive than Swaps we have:
\Theta (n^2) compares cost

If we replace pairwise swaps with binary search we get \Theta (n log n)

*** Merge Sort

Divide and Conquer, recursive algorithm

xs -> split to -> L and R -> sort -> L' and R' -> merge -> to sorted xs'

[[./img/6006/merge_sort.jpg]]

size n -> 2 unsorted size n/2 -> 2 sorted size n/2 -> size n


Merge: assumes two sorted arrays as input

L' = [20,13,7,2] and R' = [12,11,9,1]

Think of it as a two finger algorithm:
Start at one end and compare 2 elements from L' and R'
Move the smallest to the sorted array. Repeat.

#+BEGIN_SRC
>> compare 1 and 2
>> move 1 to sorted array
>> compare 2 and 9
>> move 2 to sorted array
>> compare 7 and 9
>> move 7 to sorted array
>> \dots
#+END_SRC

[[./img/6006/merge_routine.jpg]]

The merge routine does most of the job and is \Theta (n)
The overall complexity of the algorithm is \Theta (n log n)

Thm.: Merge sort is \Theta (n log n)

Proof: 

By looking at the merge routine we can construct a recurrence that looks
like this.
Complexity: T(n) = C_{1} + 2T(n/2) + C * n,

where C_{1} is the divide step, 2T(n/2) is the recursive part, and C*n is
the merge part 

C*n -> C*n/2 C*n/2 -> C*n/4 C*n/4 C*n/4 C*n/4 -> \dots -> C \dots C

the number of levels is 1+log(n), the number of leaves is n
and you're doing the same amount of work C*n at each level:

T(n) = C*n * (1 + log(n)) = \Theta (n * log n)

[[./img/6006/merge_sort_complexity.jpg]]

Merge sort needs \Theta (n) auxilary space for (L,R) and (L',R'),
Insertion sort (in place sort) needs \Theta (1) auxilary space
Consider this if you need to sort like billion of items.
There is an advanced in place merge sort algorithm, but is inpractical
with its worse running time.

In python:

Merge sort is: 2.2 * n * log(n) ms
Insertion sort is: 0.2 * n^{2} ms 

In C:
Insertion Sort is: 0.01 * n^{2} ms  


*** Recurrencies


T(n) = 2T(n/2) + C*n^{2}

C*n^{2} -> C*n^{2}/4 + C*n^{2}/4 -> C*n^{2}/16 + C*n^{2}/16 + C*n^{2}/16 + C*n^{2}/16

1+log(n) levels, n leaves,
#+BEGIN_SRC
work at leval 1: C*n^{2}
work at leval 2: C*n^{2}/2
work at leval 3: C*n^{2}/4
\dots
work at leval 1+log(n): C*n
#+END_SRC

top level C*n^{2} dominates, complexity is \Theta (n^{2})

T(n) = 2T(n/2) + \Theta (1) or C

C -> C + C -> C + C + C + C -> \dots -> n * C
#+BEGIN_SRC
work at leval 1: C
work at leval 2: 2C
work at leval 3: 4C
\dots
work at leval 1+log(n): nC
#+END_SRC

the bottom level n * C dominates, complexity is \Theta (n)

** Heaps and Heap Sort

O(n lg n) running time, sorting in place algorithm introducing new
design technique: using a DS to manage information. The heap DS is
useful for heapsort and as efficient priority queue.

Note: Heap is also used to refer to the garbage-collected storage
that langs like Java and Lisp provide. 

*** Priority Queue

Implements a set S of elements, each of elements associated with a key.
Operations: pick max or min, delete, insert, change

insert(S, x)   : insert element x into set S

max(S)         : return element of S with a largest key

extract_max(S) : as max(s), but also removes it from S

increase_key(S,x,k) : increase the value of x's key to k

*** Heap
 
xsn array visualized as a nearly complete binary tree.

[16, 14, 10, 8, 7, 9, 3, 2, 4, 1]

10 elements is not 15, so it is not a full binary tree
index 1 is the root of the tree, 2 and 3 are the children of 1

- Heap as a tree:

root of  tree: 1st element (i=1)

parent(i) = i/2

get_left(i) = 2i, right(i) = 2i + 1

xs Heap must keep the Heap property correct:
- Max-Heap property: the key of a node is >= the key of its children
- Min-Heap property: the key of a node is <= the key of its children

Heap operations:

build_max_heap : produces the max heap from an unordered array

max_heapify(xs, i) : correct a single violation of the heap property in
                    a subtree' s root


Given the asumption that the trees rooted at get_left(i) and right(i) are
max-heaps the complexity of the max_heapify is bound by the heigth of
the tree and the tree is nearly balanced binary tree => \Theta (\log n)

**** max_heapify pseudocode:

#+BEGIN_SRC ruby

def max_heapify (xs, i)
  left = get_left(i)
  right = get_right(i)

  if left <= heap_size(xs) and xs[left] > xs[i]
    largest = left
  end

  if right <= heap_size(xs) and xs[right] > xs[largest]
    largest = right
  end

  unless largest == i
    exchange(xs[i], xs[largest])
    max_heapify(xs, largest)
  end
end
#+END_SRC

#+BEGIN_SRC ruby

# build_max_heap :: [Int] -> [Int]
def build_max_heap(xs)
  middle = xs.size / 2
  
  middle.downto(1) do |i|
    max_heapify(xs, i)
  end
end

#+END_SRC

**** Analysis:

Total amount of work in the loop of max_heapify is the sum:

n/4(1 c) + n/8(2 c) + n/16(3 c) + \dots + 1(log(n) c)

Set n/4 = 2^{k} and simplify:

c 2^{k}( 1/2^{0} + 2/2^{1} + 3/2^{2} + \dots + (k + 1)/2^{k} )

bounded by a constant -> build_max_heap() is \Theta (n)

*** Min-Heap

[[./img/6006/min_heap.jpg]]

*** Heap Sort

turn unordered array into a max-heap and do extract max

Sorting strategy:
1. build Max Heap from unordered array
2. find maximum element A[1]
3. swap elements A[n] and A[1], now max element is at the end of the array
4. discard node n from heap (by decrementing heap size number)
5. new root may violate max heap property, but its children are max heaps.
Run max_heapify to fix this
6. go to step 2 unless heap is empty

** Binary Search Trees, BST Sort 

binary search is fundamental divide and conquer paradigm and there is a DS
associated with it called binary search tree.

Illustrated with a toy problem about runway scheduling.

**** Runway reservation system:
1. single runway airport
2. build for reservations for future landings
3. we want to reserve a request for landing with landing time t
4. Add t to the set of R if no other landings are scheduled within k minutes

We want the operations in: O (log n) time, where R = n

Example:

[[./img/6006/runway_scheduling_example.jpg]]

The problem: no convenient DS for the insert and search OPS

| DS                  | insert    | check     | append          | sort            |
|---------------------+-----------+-----------+-----------------+-----------------|
| Sorted list         | O(n)      | O(1)      | \Theta(n log n) | \Theta(n log n) |
| Sorted array        | \Theta(n) | O(log n)  |                 |                 |
| Unsorted list/array |           | O(n)      |                 |                 |
| Min-Heap            | O(log n)  | O(n)      |                 |                 |
| Dict/Set            | O(1)      | \Omega(n) |                 |                 |
| BST                 | O(h)      | O(log n)  |                 |                 |

**** Binary Search Tree:

unlike the heap which is just an array and we have to visualize it being a
tree, the BST is an actual tree that has pointers - parent(x), left(x) and
right(x).

What makes a tree a BST is the property that it satisfies the invariant:

#+BEGIN_SRC
for any node x, for all nodes y in the left subtree of x, key(y) <= key(x)
for all nodes y, in the right subtree of x key(y) >= key(x)
#+END_SRC

[[./img/6006/binary_search_tree_insert.jpg]]

But do we have a solution?

insertion with check is O (h) time, where h is the height of the tree
and h = n is possible

Other operations:

find_min() is O (h)
next_layer(x) is O (h)

**** Augmented Binary Tree

Client adds new requirement:
Be able to compute Rank(t) = how many planes are scheduled to land at times <=t

What lands before t?
1. Walk down the tree to find desired time
2. Add in the nodes that are smaller
3. Add in the subtrees size to the left

** AVL Trees, AVL Sort

BSTs support insert, delete, min, max, next_larger, etc in O(h) time, where
h = length of the longest path from root to leaf (down).
In the worst case for BSTs h = n, and they are more like a list.
A tree is balanced if h = O(log n)

height of a node = length (# edges) of longest downward path to a leaf

*** AVL trees (Adel'son-Vel'skii & Landis 1962)

Using the formula we can store the heigths of the nodes for free:
max(height(left_child), height(right_child)) + 1

Our goal is to keep the heights small.

nil childs are -1, so by the formula: max(-1,-1) + 1 = 0

For every node, require heights of left and right children to differ by at
most +-1 each node stores its height like subtree size or just diffrence 
in heights.

wosrt case is when the right subtree has height 1 more than the left for
every node.

N_{h} = min # nodes in an AVL

N_{0(1)} = O(1)

N_{h} = 1 + N_{h-1} + N_{h-2}, <=
(The root = 1) + (right subtree = h - 1) + (left subtree = h - 2)

looks a lot like fibonachi, just remove the 1

=> N_{h} > F_{h} = \varphi^{h}/sqrt{5}

> 1 + 2 * N_{h-2} > 2 * N_{h-2} = \Theta(2^{h/2})

h < 2 \log_{2}(n) => \Theta(log n) 

How do we maintain the property?

AVL insert:
1. Simple BST insert
2. work your way up tree, restoring AVL property(and updating heights)

Rotation: 

Left rotate:

[[./img/6006/avl_left_rotate.jpg]]

Right rotate:

[[./img/6006/avl_right_rotate.jpg]]


*** AVL Sort

very powerful way to sort in O(n log n)

- insert n items - \Theta(n log n)
- in-order traversal - \Theta(n)

** Counting Sort, Radix Sort, Lower Bounds for Sorting and Searching

We start with a Thm, a proof and a counter example. We argue that the lower
bound for searching is \Omega(\log(n)) and for sorting is \Omega(n \log n),
but we see that we can get away with just n for some special cases. 

*** Claims:

1. searching among n preprocessed items requires \Omega (log n) time:
binary search, AVL tree search are optimal

2. sorting n items requires \Omega(n log n):
mergesort, heap sort, AVL sort are optimal

*** Comparison Model

A new model of computation useful for proving lower bounds.

- Lets restrict the kind of operations to be only comparisons:
( <, <=, >, >=, = ), only binary answer yes or no. 
 
- All input items are black boxes, we don't know what they arei(ADTs).

- Time cost is only the # comparisons

**** Decision Tree

If know that our algorithm is only comparing items we can draw all the
possible things that an algorithm can do.
Any comparison algorithm can be viewed/specified as a tree of all possible
comparison outcomes and resulting output, for a particular n.

Example: binary search for n = 3

1. internal nodes = binary decisions
2. leafs = algorithms is done(output)
3. root-to-leaf path = algorithm execution
4. path length (depth) = running time
5. height of tree = worst-case running time

Binary decision tree is more powerful than comparison model,
and lower bounds extend to it

[[./img/6006/decision_tree.jpg]]

*** Lower Bounds
    
**** Searching lower bounds

Thm:
Given n preprocessed items, meaning we get sorting or structuring in a heap
for free, to find a given item among them in comparison model requires
\Omega(\log(n)) in worst case.

Pf:
Decision tree is binary and it must have >=n leaves, one for each answer.
-> height >= \log(n)

1. # leaves >= # possible answers >= n
2. decision tree is binary
3. -> height >= log \Theta(n) = log n \pm \Theta(1)

**** Sorting lower bounds

A[i] < A[j] -> Yes or No

Decision tree is binary and # leaves is atleast the # of possible answers
(answer may appear in several leaves) = n!

-> height is >= \log(n!)

Use Sterling formula or Sum to proove that it is n \log(n)

1. leaf specifies answer as permutation: A[3] <= A[1] <= A[9] <= \dots 
2. all n! are possible answers
3. # leaves >= n!
   
4. in fact \log(n!) = n \log(n) - O(n) via Sterlings formula:

-> height >= \log(n!)
= \log(1 * 2 * \dots * (n - 1) * n)
= \sum_{i=1}^{n} \log(i)
>= \sum_{i=1}^{n/2} \log(i)
>= \sum_{i=1}^{n/2} \log(n/2)
= n/2 * \log(n) - n/2
= \Omega(n \log n)

n! ~ \sqrt{2 \pi n}(n/e)^{n} -> \log(n!) ~ n \log(n) - O(n)

*** Linear-Time Sorting (special case for sorting Integers)

Lets use the RAM model here. We can do more than only comparisons.

\O(n \sqrt{\log \log n}) is the best algorithm here, but when n is small
it is possible to do it in linear time.

**** Counting Sort:

[3,5,7,5,5,3,6]

1. Count the items     

[3, 3, 5, 5, 5, 6, 7]

2. \dots

Those kind of algorithms sort only Integers, but those Integers can carry
other stuff with them.

#+BEGIN_SRC 
L = array of k empty lists

for j in n
  L[key(A[j])].append(A[j])

output = []

for i in k
  output.extend(L[i])

#+END_SRC

**** Radix Sort: 

For sorting numbers. Sorts the numbers from least to most significant digit.

we need n buckets and k passes over the numbers,
where k is the # digits of the bigest number.

xs = [10, 15, 1, 60, 5, 100, 25, 60]

1. find largest number -> 100
2. count # digits in largest number -> 3
3. right pad the rest of the numbers ->

[010, 015, 001, 060, 005, 100, 025, 060]

4. pass 1: sort by using the 1st digit only ->

[010,060,100,050], [001], [], [],[],[015,005,025], [], [], [], []

take numbers out of buckets strating from bottom (1st number in array) ->

[010,060,100,050,001,015,005,025]

5. pass 2: sort by using the 2st digit only ->

[100,001,005], [010,015], [025], [], [],[050], [060], [], [], [] ->

[100,001,005,010,015,025,050,060]

6. pass 3: sort by using the 3st digit only ->

[001,005,010,015,025,050,060], [100], [], [], [], [], [], [], [], [] ->

[001,005,010,015,025,050,060,100]

7. remove the leading zeros

** Haching I: Chaining

*** Dictionary

ADT maintaining a set of items, each with a key. Perhaps the most popular
data structure is CS.

- build into most languages
- databases use tree search or hashing
- compilers and interpreters(mostly old ones)
- network routers, network server, virtual memory
- substring search
- string commonalities
- cryptography

OPS:

- insert(item) - add item to set
- delete(item) - remove item from set
- search(key)  - return item with key if it exists

We assume items have distinct keys.

Balanced BSTs solve in O(\log n) time per op.
Our goal is O(1) per operation.

**** Direct Access Table

Items stored in an array indexed by keys(random access)

Problems:
1. keys must be nonnegative Integers (or using two arrays, Integers),
it is hard to associate something with an Integer
2. large key range -> large space - e.g. one key of 2^{256} is not good,
it is gigantic memory hog

Solutions:

Solution to 1 is prehashing:

maps keys to nonnegative Integers

- In theory, possible because keys are finite -> set of keys is countable
- In Python: hash(object), where object is a number, string, tuple, etc. or
object implementing \__hash__, maps the thing to an Integer
- In theory, x = y -> hash(x) = hash(y)
- Python applies some heuristics for practicality: for example,
hash('\0B') = 64 = hash('\0\0C')
- Object's key should not change while in table (else cannot find it anymore)
- No mutable objects like lists 

Solutions to 2 is hashing:

We have a giant key space \U and if we store it in the direct access table,
this will also be giant, so we want to map it using a hash function h down
to some smaller set m the size of our hash table. Then there is a subset
of \U with keys that are actualy stored in the dictionary. That set changes.
The idea is that we would like m to be around n, m = \Theta(n), so will use
linear space O(n).

The problem is that in this case space m will be too small and there might
be collisions.

- Reduce universe \U of all keys(say, Integers) down to reasonable size m for table
- idea: m ~ n = # keys stored in dictionary
- hash function h: \U -> { 0, 1,..., m - 1 }
- two keys k_{i}, k_{j} \in K collide if h(k_{i}) = h(k_{j})

How do we deal with collisions?
1. Chaining
2. Open addressing

*** Chaining   

If you have colliding items, just store them in a linked list.

- The search must go through the whole linked list T[h(key)]
- Worst case: all n keys hash to same slot -> \Theta(n) per op
In the case of hashing randomization helps to keep you well away from the
worst case analysis.

[[./img/6006/hashing_with_chaining.jpg]]  

Expected cost of insert/delete/search is O(1 + \alpha)

*** Simple Uniform Hashing

An assumption (cheating): each key is equally likely to be hashed to any
slot of table, independant of where other keys are hashed.

let n = # keys stored in table,
let m = # slots in table

load factor \alpha = n/m = expected # keys per slot = expected length of a chain

Performance:

This implies that running time for search is \Theta(1 + \alpha), where the 1
comes from appling the hash function and random access to the slot whereas
the \alpha comes from searching the list. This is equal to O(1) if
\alpha = O(1), i.e., m = \Omega(n)

*** Hash Functions
A good hash function satisfies the assumption of simple uniform hashing:
each key is equally likely to hash to any  of the m slots, independantly
of where any other key has hashed to. This is hard to check, because we
rarely know the distibution from which the keys are drawn.

**** Division Method:

h(k) = k mod m, gives you a umber between 0 and m-1

In most situation it is a bad choice.
It can be practical when m is prime, but not too close to power of 2 or 10.
But it is inconvenient to find a prime number, and division is slow.

**** Multiplication Method:

h(k) = [(a * k) mod 2^{w}] >> (w - r)

, where >> means shift right
, w is w bit machine from models of computations

where a is random, k is w bits, and m = 2^{r}. This is practical when a is
odd and 2^{w-1} < a < 2^{w} and a is not too close to 2^{w-1} or 2^{w}.

Multiplication and bit extraction are faster than division.
But in theory this method is also bad.

**** Universal Hashing

Now for the cool one.

For example:

h(k) = [(ak + b) mod p] mod m,

where a and b are random \in {0,1,\dots,p-1}, and p is a large prime (> |\U|)  

This implies that for worst case keys k_{1} != k_{2}
the probability of 2 them colliding is 1/m:

Pr_{a,b}{event X_{k_{1}k_{2}}} = Pr_{a,b}{h(k_{1}) = h(k_{2})} = 1/m

This implies that:

E_{a,b}[# collisions with k_{1}] = E[\sum_{k_{2}} X_{k_{1}k_{2}}]

= \sum_{k_{2}} E[X_{k_{1}k_{2}}]

= \sum_{k_{2}} Pr{ X_{k_{1}k_{2}} = 1}

= n/m = \alpha

** Hashing II: Table Doubling, Karp-Rabin

How to choose m, so that it is \Theta(n), -> \alpha is \Theta(1)?

Idea:
Start small with a constant (power of 2 is ok) and grow (or shrink)
as necessary. When m > n grow the table. You need to allocate the memory
and rehash.

Grow table: m -> m'
- make table of size m'
- build new hash h' function
- rehash

#+BEGIN ruby

table.each { |item| table_prime.insert(item) }

#+END_SRC

For every item in the table you have to look at every slot, so you have to
pay O(m) to visit every slot, O(n) to visit all those lists and m' to build
the new table -> \Theta(n + m + m') -> \Theta(n)

Rehashing:
To grow or shrink, table hash function must change (m, r).
-> must rebuild hash table from scratch
-> \Theta(n + m) time = \Theta(n) if m = \Theta(n)

*** Table Doubling

How much to grow the table?

m' = 2m, is ok,

you have to think about the cost of insertion here, because every time you
do this you add O(n) cost to the usual O(1) cost.

If we grow too little, say each step cost is big:
- m' = m + 1 ?
-> rebuild every step
-> n inserts cost \Theta(1 + 2 + \dots + n) = \Theta(n^{2})

If we double m:
- m'= m * 2? m = \Theta(n) still (r +=1 )
-> rebuild at insertion 2^{i}
-> n inserts cost \Theta(1 + 2 + 4 + 8 + \dots + n), where n is the next power of 2
-> = \Theta(n)

Now some inserts cost O(n), but all other inserts are O(1),
so we say that insertion is O(1) "on average". From here comes the idea of
amortization.

*** Amortization

Amortized Analisys:
You spread out the higher cost so that it is cheap on average.

- operation has amortized cost T(n) if k operations cost <= k * T(n)
- "T(n) amortized" roughly means T(n) "on average", averaged over all ops.
- e.g. inserting into a hash table takes O(1) amortized time.

Deletion:

What happens when we have to shrink the table?

1. if m = n/2 then shrink to m/2

The problem is that this will trigger a cycle of shrinking and growing.
8 items + 1 -> you grow to 16, 9 - 1 -> you shrink to 8, and you pay O(n).

2. if m = n/4 then shrink to m/2

amortazied time is \Theta(1), and you maintaon the invariant n <= m <= 4n 

Also O(1) expected as is.
- space can get big with respect to n e.g. n x insert, n x delete
- solution when n decreases to m/4, shrink to half the size
-> O(1) amortized cost for both insert and delete

Note: It is possible to get rid of the amortized and do it in wosrt case
constant time. When the table is getting full, start building on the side
a new table that is twicw the size and every time you insert into the main
table you move 5 items to the new table(or big enough constant), so by the
time the main is full you switch imidiatly to the new table.
Useful in real-time sysytems.

Back to hashing:

maintain m = \Theta(n) -> \alpha = \Theta(1) -> support search in O(1)
expected time (assuming simple uniform or universal hashing)

Resizable Arrays:

- same trick solves Python "list" (array)
- -> list.append and list.pop are O(1) amortized

*** Karp-Rabin Algorithm:

**** Substring Matching:

Lets first examine the problem of substring matching:
Given 2 strings s and t, does s occur as a substring of t?
(and if so, where and how many times)

E.g. s = 6.006 and t = your entire INBOX ('grep' on UNIX) 

Simple Algorithm:

#+BEGIN_SRC 

any(s == t[i:i + len(s)])
    for i in (len(t) - len(s))

#+END_SRC

- O(|s|) time for each substring comparison
-> O(|s| * (|t| - ||s)) time = O(|s| * |t|), potentialy quadratic

If we use hashing we can get it down to linear time O(|s| + |t|).
We are looking for rolling window of t always of size s, and each time we
want to know is it the same as s, we check instead of the strings a hash
function of the strings. The Universe of strings is big, but if we can hash
it down to some reasonable size, something that fits in a word
(models of computation), we can compare those 2 words, hash values are
equal, whether there is a collision in the table. This will be O(1) per op.


**** Rolling Hash ADT:

We can try doing this with a rolling hash, but we need a DS for this.
- Given a rolling hash value r we want op append(c) a character at the end
of x,
- and operation skip(c) to delete the first char of x (assuming it is c)
- r() is op to give hash value of x = h(x)

Karp-Rabin string matching algorithm:

- compute hash function of s
- compute hash function of first s chars of t
- check if those hashes are equal,
- if not add 1 char at the end, delete 1 char from the beginning

#+BEGIN_SRC ruby

rs # rolling hash of r
rt # rolling hash of t

s.each { |c| rs.append(c) }

t.each { |c| rt.append(c) }

if rs() == rt()
  [s.size..t.size].each do |i|
    rt.skip(t[i - s.size])
    rt.append(t[i])
    if rs() == rt()

    end
  end
end 

#+END_SRC

- Compare h(s) == h(t[i:i + len(s)])
- if hash values match, likely so do strings
  - can check s == t[i:i + len(s)] to be sure ~ cost O(|s|)
  - if yes, found match - done
  - if no, happend with probability < 1/|s| -> expected cost is O(1) per i
- need suitable hash function
- expected time = O(|s| + |t| * cost(h))
  - naively h(x) costs |x|
  - we'll achive O(1)!
  - idea: t[i:i + len(s)] ~ t[i+1 : i+1 + len(s)]


TODO: can't get good explanation of the whole algorithm from the lecture,
or the book, research and make one.


** Hashing III: Open Addressing, Criptographic Hashing

*** Open Addressing   

    Another approach to collisions:

Make a hash and probe for place if not make new hash and probe again.

- no chaining, instead all items are stored in table
- one item per slot -> m>=n
- hash function specifies order of slots to probe(try) for a key
(for inseart/search/delete), not just one slot.
We want to design a function h, with property that for all k \in U:

h: U \cross {0,1,\dots,m - 1} -> {0,1,\dots,m - 1}

universe of keys --- trial count --- slot in table

{ h(k,0), h(k,1), \dots, h(k,m-1) }

i.e. if you keep trying h(k,i) for every increasing i, you will
hit all slots of the table.


insert(k,v): Keep probing until an empty slot is found.

search(k): As long as the slots you encounter by probing are occupied
by keys != k, keep probing until you either encounter k or find an
empty slot.

delete(): can't just find item and remove its slot,
you have to replace item with special flag "Delete Me", which insert
treats as None, but Search doesn't.

*** Probing Strategies 

**** Linear Probing
     h(k,i) = (h'(k') + i) mod m, where h'(k) is ordinary hash function
     
Like street parking. The problem is clustering, consecuitive group of
occupied slots as cluster become longer, it gets more likely to grow
further. Can be shown that for 0.01 < \alpha < 0.99 say, clusters of
size \Theta(\log n).

**** Double Hashing
     h(k,i) = (h_{1}(k) + i * h_{2}(k)) mod m,

where h_{1}(k) and h_{2}(k) are 2 ordinary hash functions

actually hit all slots (permutation) if h_{2}(k) is relatively prime
to m for all k.

e.g. m = 2^{r} make h_{2}(k) always odd

*** Uniform Hashing Assumption
    Each key is equally likely to have any one of the m! permutations
as its probe sequence. Not really true, but double hashing may come
close.

Suppose we have used open addressing to insert n items into table
of size. Under the uniform hashing assumption the next operation
has expected cost of <= 1/(1-\alpha), where \alpha = n/m (<1).

\alpha = 90% -> 10 expected probes


Pf:
Suppose we want to insert an item with key k. Suppose that the item is
not in the table.
- probability first probe successful:

m-n/m =: p

(n bad slots, m total slots, and first probe is uniformly random)

- if first probe fails, probability second probe succesful:

m-n/m-n >= m-n/n = p

- if 1st and 2nd probe fail, probability 3rd probe successful:

m-n/m-2 >= m-n/m = p

(since two bad slots already found, m - n good sots remain and the
3rd probe is uniformly random over the m - 2 total slots left)

- \dots
  
-> Every trial, success with probability at least p.
Expected number of trials for success:

1/p = 1/1-\alpha

-> search and delete takes time O(1/1-\alpha)


*** Open Adressing vs. Chaining
    - Open AddressingL better cache performance (better memory usage,
no pointers needed)

    - Chaining: less sensitive to hash functions (OA requires extra
care to avoid clustering) and the load factor \alpha (OA degrades
past 70% or so and in any event cannot support values larger than 1)

*** Cryptographic Hashing

**** Properties
     - One-Way(OW): infeasible, given y \in \R {0,1}^{d} to find any x s.t.
h(x) = y. This maens that if you choose a random d-bit vector, it is hard
to find an input to the hash that produces that vector. This involves
"inverting" the hash function.

     - Collision-resistance (CR): Infeasable to find x, x', s.t. x!=x' and
h(x) = h(x'). This is a collision, two input values have the same hash.

     - Target collision-resistance (TCR): infeasible given x to find
x' = x s.t. h(x) = h(x').

TCR is weaker than CR. If a hash function satisfies CR, it automatically
satisfies TCR. There is no implication relationship between OW and CR/TCR.


**** Applications

     - password storage: Store h(PW), not PW on computer. When user inputs PW,
compute h(PW') and compute against h(PW). The property required of the hash
function is OW. The adversary does not know PW or PW' so TCR or CR is not
really required. Of cource, if many many passwords have the same hash, it is
a problem, but a small number of collisions is ok.

     - file modification detector: for each file F, store h(F) securely.
Check if F is modified by recomputing h(F). The property that is required
is TCR, since the adversary wins if he is able to modify F without changing
h(F).

     - digital signitures: in public key cryptography, Alice has a public
key PK_{A} and a private key SK_{A}. Alice can sign a message M using her
private key to produce \sigma = sign(SK_{A}, M). Anyone who knows Alice's
public key PK_{A} and verify Alice's signature by checking that
verify(M, \sigma, PK_{A}) is true. The adversary wants to forge a signiture
that verifies. For large M it is easier to sign h(M) rather than M, i.e.,
\sigma = sign(SK_{A}, h(M)). Property is CR. We don't want an adversary
to ask Alice to sign x and then claim she signed x', where h(x) = h(x').

**** Implementaions

Cryptographic hash functions are significantly more complex than those
used in hash tables. Think of it as a regular hash running many, many
times with pseudo-random permutations interspersed.

** Numerics I: Integer Arithmetic, Karasuba

*** Digression: Machine Epsilon - \epsilon_{mach}

\epsilon_{mach} is difined as the smallest number such that 1 + \epsilon_{mach} > 1.
It is the difference between 1 and the next nearest number representable
as a machine number.

In a hypothetical word that stores 8 bits:
- 1st bit - is used for the sign of the number
- next 3 bits - for the biased exponent
- last 4 bits - for the magnitude of the mantissa.

1_{10} = [0,0,1,1,0,0,0,0]

The next number in the binary word would be:

1.0625_{10} = [0,0,1,1,0,0,0,1]

\epsilon_{mach} = 0.0625 

It is the upper bound on the absolute relative true error in
representation of a number x.

*** Irrationals
Every once in a while you will have a situation where you want to
compute with numbers much longer than 64 bits(the word length in
today's standart computer). You want to find precisely the weight
of a nutrino, literaly 100 decimal digits.

with k bits you can represent 2^{k} values, but their range will
depend on the system you are using:

Unsigned: 0 to 2^{k} - 1,
Signed: -2^{k-1} to 2^{k-1}-1

2^{64} = 18_446_744_073_709_551_616 -> 

What if you want the \sqrt{2} to million digits or \pi, how do
you do that on a computer?
In cryptograpy(RSA) you have to deal with really bbig prime
numbers, that are 1000 of bits long and do division and
multiplication on them.

Pythagoras discovered that a square's diagonal and its side are
incommensurable, you could not expressed the ratio as a rational
number - ratio of Integers(Babylonians and Indeans new about this
before him, but he gets credited for the Pythagoras Theorem).

All is Number - the Pythagorean religion.

He didn't like the \sqrt{2}, because he could not express it as a
number and called it "speechless". They tried to find patterns in
Irrationals, but could not succeed(this would be better than p =np).
Irrational numbers were evil.

\sqrt{2} = 1.414_213_562_373_095_048_801_688_724_209_698_078_569_671_875

**** Digression - Catalan numbers:

How many ways to select 2 balls from a box contining 9 balls?

n choose k -> 9 choose 2 -> n!/n!(n - k)! = 9!/9!(9-2)! = 9*8/2*1 = 36

How many ways to choose a 3-topping pizza based on 10 toppings menu?

10 choose 3 = 10!/7!3! = 10*9*8/3*2*1 = 120

Cardinality of the set P of balanced parentheses string are recursively
defined as:
- \lambda \in P, where \lambda is the empty string
- If \alpha, \beta \in P, then (\alpha)\beta \in P

Every nonempty balanced paren string can be optained via Rule 2 from a
unique \alpha, \beta pair. For Example: (()) () ()

**** Enumeration
C_{n}: number of balanced parentheses strings with exactly n pairs of
parentheses. C_{0} = 1 empty string

C_{n+1}? Every string with n + 1 pairs of parentheses can be obtained
in a unique way via Rule 2.

One paren pair comes explicitly from the rule.
kpairs from \alpha, n - k pairs from \beta

C_{n+1} = \sum_{k=0}^{n} C_{k}C_{n-k}, n >= 0

*** Newton's Method

Find root of f(x) = 0, through succesive approximation e.g., f(x) = x^{2} - a

x_{i+1} = x_{i} - f(x_{i}) / f'(x_{i})

Square roots:

f(x) = x^{2} - a

x_{i+1} = x_{i} - (x_{i}^{2} - a)/2x_{i} = (x_{i} + a/x_{i}) / 2

a = 2

x_{0} = 1.000000000
x_{1} = 1.500000000
x_{2} = 1.416666666
x_{3} = 1.414215686
x_{4} = 1.414213562

Quadratic convergence, # digits doubles


*** High Precision Computation

\sqrt{2} to d-digit precision: 1.414213562373 } d digits


High Precision Multiplication


*** Karatsuba's Method

[[./img/6006/karatsubas_method.jpg]]

    TODO: finish it

** Graphs I: Breadth First Search

*** Graphs

Graph G = (V,E)
- V = set of vertices,
- E = set of edges, vertex pairs
  - ordered pair -> directed graph
  - unordered pair -> undirected graph

[[./img/6006/graphs.jpg]]

How do you represent a graph like this for an algorithm?
If you just use arrays for the 2 sets, you will need O(n) to just find
the neighbours of a.

Graph Search is used in exploration problems:
- find a path from start vertex s to a desired vertex
- visit all vertices or edges of graph, or only those reachable from s

Applications:
- web crawling, indexing pages on the internet
- social networking, friends and friends of friends
- garbidge collection
- checking mathematical conjectures
- solving puzzles and games

*** Pocket Cube

Consider a 2 by 2 by 2 Rubik's cube.

Configuration Graph:
- vertex for each possible state
- edge for each basic move(90 degree turn) from one state to another
- undirected: moves are reversible

Diameter of a graph ("God's Number" or worst case of moves needed to solve)

The best way to compute it is to construct this graph one layer at a time
until you are done and then you know what the diameter is. The trouble is
the layers in between grow exponentially, at some point it decreases, but
getting over this exponential hump is really hard. Even for 3 by 3 by 3
they used a lot of tricks to speed up the algorithm, but in the end it is
esentially a breadth-first search.

11 for 2 by 2 by 2,  
20 for 3 by 3 by 3,  
\Theta(n^{2} / \log n) for n by n by n

the # vertices = 8! * 3^{8} = 264_539_520, where 8! comes from having
8 cubelets in arbitrary positions and 3^{8} comes as each cubelet has
3 possible twists.

This can be divided by 24 if we remove cube symmetries and further
divided by 3 to account for actually reachable configurations, there
are 3 connected components.

*** Graph Representations: DS

**** Adjacency Lists

There is one representation called adjacency list with many variations.

You have an array called Adj of size V, each element is a pointer to a
linked list. The idea si that this array is indexed by a vertex.
It cound be index by nubers or by hashable objects in which case Adj
is a hash table. For every vertex you store its neighbours, namely the
verteces you can reach by one step.

Algorithms must run in O(V + E) time.

Array Adj of V linked lists
- for each vertex u \in V, Adj[u] stores u's neighbors,
{v \in V | (u,v) \in E}.
(u, v) are just outgoing edges if directed

[[./img/6006/graph_list_representation.jpg]]

- in Python:
Adj = dictionary of list/set values;
vertex = any hashable object (int, tuple)

- advantage: multiple graphs on same vertices

**** Implicit Graphs

Adj(u) is a function - compute local structure on the fly(Rubik's Cube).
This requires "Zero" space. Really useful for the case of Rubik's cubes.

**** Object-oriented Variations

- object for each vertex u
- u.neighbors = list of neighbors, Adj[u]

If you want just one graph from the sets of vertecies and edges.

**** Incidence Lists

- can also make edges objects

[e.a] -> e -> [e.b]

- u.edges = list of outgoing edges from u
- advantages: store edge data without hashing

*** Breadth-First Search

We want to visit all the nodes reachable from given s \in V,
in O(V + E) time.    

Explore graph level by level from s
- level 0 = {s}
- level i = vertices reachable by path of i edges but not fewer
- build level i > 0 from level i-1 by tryin all outgoing edges,
but ignoring vertices from previous levels

Breadth-First-Search Algorithm:

Pseudocode based on ruby:
#+BEGIN_SRC ruby

def bfs(v, adj, s)

  level = { s: 0 }
  parent = { s: nil }
  i = 1
  frontier = [s] # previous level, i-1
  
  while frontier
    next = []    # next level. i

    frontier.each do |u|
      
      adj.each do |v|
        if v not in level # not yet seen
          level[v] = i    # number = level[u] + 1 
          parent[v] = u
          next.append(v)
        end   
      end
    end

    frontier = next
    i = i + 1
end 

#+END_SRC

Analysis:

Vertex V enters next and then frontier only once, because level[v]
then set.

Base case: v = s -> Adj[v] looped through only once

time = \sum_{u \in V} Adj[V] = { E for directed graph; 2E for undirected

-> O(E) time, O(V+E) to also list vertices unreachable from v(not assigned level)

Shortest Paths:

the pointers to parent form the shortest paths.

- for every vertex v, fewer edges to get from s to v is:
  
{ level[v] if v assigned level; \inf else (no path)

- parent pinters from shortest-path tree = union of such a shortest
path for each v -> to find shortest path,

take v, parent[v], parent[parent[v]], etc until s (or None)

** Graphs II: Depth-First Search

This is like exploring a maze.
- follow path until you get stuck
- backtrack along breadcrumbs until reach unexplored neighbor
- recursively explore
- careful not to repeat a vertex

#+BEGIN_SRC ruby

parent = { :start => nil }

def dfs_visit(verteces, adj, start)
  adj[start].each do |vertex|
    unless parent.has_key?(vertex)
      parent[vertex] = start
      dfs_visit(vertices, adj, vertex)   
    end
  end
end

def dfs(vertices, adj)
  parent = {}
  vertices.each do |s|
    unless parent.has_key?(s)
      parent[s] = nil
      dfs_visit(verteces, adj, s);
    end
end

#+END_SRC

**** Edge Classification

- to compute this classification (back or not), mark nodes for duration
they are "on the stack"
- only tree and back edges in undirected graph

**** Analysis

- dfs_visit gets called with a vertex s only once, because then
parent[s] set -> time in dfs_visit = \sum_{s \in V} Adj[s] = O(E)

- dfs outer loop adds just O(V) -> O(V+E) time

*** Cycle Detection

Graph G has a cycle <-> DFS has a back edge

- before visit to v_{i} finishes will visit v_{i+1} and finish:
will consider edge (v_{i}, v_{i+1}) -> visit v_{i+1} now or already did

- before visit to v_{0} finishes, will visit v_{k} and did't before

- before visit to v_{k} (or v_{0}) finishes, will see (v_{k}, v_{0})
as back edge

** Shortest Paths I: Intro

Shortest way to drive from A to B
Problem on a weighted graph G(V,E) W: E -> R

Two algorithms:
- Dijkstra     : O(V log V + E), assumes non-negative edge weights
- Bellman Ford : O(V E), is a genral algorithm

V = vertices (strict intersections)
E = edges (street, roads); directes edges (one way roads)
W(U,V) = weight of edge from u to v (distance, toll)

path p = < v_{0}, v_{1}, \dots, v_{k} >
(v_{i}, v_{i+1}) \in E for 0 <= i <= k

w(p) = \sum_{i=0}^{k-1} w(v_{i}, v_{i+1})

*** Weighted Graphs

v_{0} - p -> v_{k} ,
means p is a path from v_{0} to v_{k}.
(v_{0} is a path from v_{0} to v_{0} of weight 0)

Shortest path weight from u to v as:

\delta(u, v) = { min { w(p): u - p -> v } if \exists path; \inf else unreachable }

 


*** General Approach

*** Negative Edges

*** Optimal Substructure

* 6.851 - Advaced Data Structures

** Persistent Data Structures
   data structures where we keep all information about past states. They
are part of the larger class of temporal data structures.

Usually we deal with data structure updates by mutating something in
existing data structure: either its data or the pointers that organize it.
In this process we lose information about previous states. Persistant
data structures do not lose any information.

It is possible to transform a plain data structure into a persistent one
with asymptotically minimal extra work or space overhead.

A recurring theme in this area is that the model is cruicial to the
results.

Partial and full persistance correnspond to time travel with a branching
universe model.

Two groups of temporal DSs:
- persistence - branching universe, you never destroy, just create new.
- retroactivity - 'back to the future', change in past reflects present. 

*** Pointer Machine Model of DSs
    Corresponds to OOP. Here we think of DSs as collections of nodes of
a bounded size with entries for data. Each piece of data in the node can
be either actual, or a pointer to a node. The primitive operations
allowed in this model are:

- x = new Node()
- x = y.field
- x.field = y
- x = y + z
- destroy(x),

where x, y, z are names of nodes or fields in them.
You have a root node and always work relative to the it.

DSs implementable with these shape constraints and operations include
linked lists and binary search trees, and in general corresponds to
struct's in C or objects in Java. An example of DS not in this group
would be a one with variable size such as an array.

*** Definitions of Persistence

1. Partial Persistence (Deja Vu) - we may query any previous version of
the DS, but we may only update the latest version. Implies linear ordering
on the versions. Ops:
- read(var, version);
- newversion = write(var, val);
2. Full Pesistence (branching universe) - both updates and queries are
allowed on any version of the DS. The versions form a branching tree. Ops:
- read(var, version);
- newversion = write(var, version, val);
3. Confluent Persistence (Sliders, Merging Universies) - in addition to
the previous ops, we allow combination ops to combine input of more then
one previous version to output a new single version. Combinations of
versions induce a direct acyclic graph on the verion graph. Ops:
- read(var, verison);
- newversion = write(var, version, val);
- newversion = combine(var, val, version1, version2);
4. Functional Persistence - from fp, where objects are immutable.
The nodes in this model are likewise immutable: revisions do not alter the
existing nodes in the DS, but create new ones instead. Okasaki book.

The difference between functional persistence and the rest is we have to
keep all the previous versions. The only allowed internal operation is to
add new nodes. 

[[./img/6851/persistence.jpg]]

*** Partial Persistence
    Is it possible to implement partial persistence efficiently?
    Yes. Assuming pointer machine memory model and restricting in-degree
    of data nodes to be O(1).
    
    Any pointer-machine DS with a constant # pointers to any node
    (in-degree, pointers out and into a node) can be transformed into 
    another DS that is partially persistent with O(1) amortized factor
    overhead and O(1) space per change in the DS.

    Pf.:
    - store back pointers:
      When you have a pointer out to a node, you want a back pointer that
      points back, so you know where all the pointers came from.
    - store mods. to fields of DS (log modifications):  
      if p is the in-degree to a node, we allow 2p modifications stored
      - modification(version, field, value):
        - version that got changed
        - field that got changed
        - value it got changed to
     
To read node.field at version v look at mods. with version <= v
The hard part is how do you change a field, because there might not be a
room in the mods structure.
To set node.field = x, we check if there is any space in mods. we just
add a mod. Else if the node is full we need to make a new node' with all
the mods applied. 



    

