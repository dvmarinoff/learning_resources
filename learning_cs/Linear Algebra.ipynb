{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "Vectors have leght and magnitude.\n",
    "#### Length of a vector:\n",
    "\n",
    "$$ \\vert \\vec{A}\\vert = \\sqrt{a_{1}^{2} + a_{2}^{2} + a_{3}^{2}} $$\n",
    "\n",
    "#### Vector addition:\n",
    "\n",
    "$$ \\vec{A} + \\vec{B} = < a_{1} + b_{1},\\, a_{2} + b_{2},\\, a_{3} + b_{3} > $$\n",
    "\n",
    "#### Dot Product:\n",
    "\n",
    "Vector $ \\to $ Scalar operation\n",
    "\n",
    "$$ \\vec{A} + \\vec{B} = \\sum a_{i}b_{i} = a_{1} + b_{1},\\, a_{2} + b_{2},\\, a_{3} + b_{3} $$\n",
    "\n",
    "Theorem:\n",
    "\n",
    "$$ \\vec{A} \\cdot \\vec{B} = \\vert \\vec{A}\\vert \\, \\vert \\vec{B}\\vert \\cos\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Combinations, Span, Basis vectors\n",
    "\n",
    "\"Linear Algebra revolves around vector addition and scalar multiplication.\"\n",
    "\n",
    "You can think of the vector coordinates in terms of scalar multiplication.\n",
    "When you have a pair of numbers that is meant to describe a vector, you may think of each coordinate as a scalar.\n",
    "How each one stretches and squishes vectors.\n",
    "\n",
    "In the xy-coordinate system the $ \\hat{i} $ and $ \\hat{j} $ are the two special vectors, know as basis vectors.\n",
    "\n",
    "$$ \\hat{i} = \\begin{bmatrix} 1\\\\ 0\\end{bmatrix}\\text{ and } \\hat{j} = \\begin{bmatrix} 0\\\\ 1\\end{bmatrix} $$\n",
    "\n",
    "Lets say: $$ \\text{ } \\vec{v} = \\begin{bmatrix} 3\\\\ -2\\end{bmatrix} $$\n",
    "\n",
    "Now you can think of the x-coordinate as a scalar that scales $ \\hat{i} $, stretching it by a factor of 3\n",
    "making: $ x = 3 \\hat{i} $ and $ y = -2 \\hat{j} $. In this sense, the vectors that these coordinates describe is the sum of two \n",
    "scaled vectors $ (3)\\hat{i} + (-2)\\hat{j} $.\n",
    "\n",
    "Can we choose different basis vectors and still have a usable coordinate system.\n",
    "For example take some vector $ \\vec{v} $ and $ \\vec{w} $, and think of all the vectors you can get by choosing two \n",
    "scalars, using each one to scale one of the vectors, then add them together what you get. You can reach every \n",
    "possible 2d vector. A new pair of basis vectors like this still gives us a valid way to go back and forth between\n",
    "pairs of numbers and 2d vectors, but the association is different from the one you get usiong the more standard basis\n",
    "vectors.\n",
    "\n",
    "Any time we are scaling two vectors and adding them together its called linear combination of those two vectors.\n",
    "\n",
    "$$ \\text{ Linear combination } (\\vec{v}, \\vec{w}) = a \\vec{v} + b \\vec{w} $$\n",
    "\n",
    "If we fix one of those two scalars and leave the other change its value freely, the tip of the resulting vector will\n",
    "draw a straigth line (hence linear), if you let both scalars range freely, and consider every possible vector that\n",
    "you can get there are 3 things that can happen:\n",
    "\n",
    "- for most pairs of vector you will be able to reach evety possible point in the plane\n",
    "\n",
    "- however if the two vectors happen to line up the tip of the resulting vector is limited to just a single line\n",
    "passing through the origin\n",
    "\n",
    "- if both vectors are 0, than you are stuck at the origin\n",
    "\n",
    "The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is called the\n",
    "span of those two vectors.\n",
    "\n",
    "$$ \\text{ Span }(\\vec{v_{1}}, \\vec{v_{2}, \\dots}, \\vec{v_n}) = \\{ c_{1}\\vec{v_{1}}, c_{2}\\vec{v_{2}}, \\dots, c_{n}\\vec{v_{n}} \\text{ | } C_{i} \\in \\mathbb{R} \\text{ for } 1 \\leq i \\leq n \\} $$\n",
    "\n",
    "Vectors vs Points\n",
    "\n",
    "It gets reaaly crowded to think about a whole collection of vectors sitting on a line, and even more to think about\n",
    "all 2d vectors all at once, filling up the plane. So when dealing with collections of vectors like this it is common \n",
    "to represent each one as just point in space. The point at the tip of that vector, where, as usual you are thinking \n",
    "about that vector with its tail on the origin. That way, if you want to think about every possible vector whose tip\n",
    "sits on a certain line, just think about the line itself. Likewise 2d vectors as a point where the tip sits - in \n",
    "effect you get the infinite flat sheet of 2d space itself.\n",
    "\n",
    "In general, if you are thinking about a vector on its own, think of it as an arrow, and if you are dealing with a\n",
    "collection of vectors, its convenient to think of them as points.\n",
    "\n",
    "The idea of Span gets more interesting in 3d space. For Example if you take 2 vectors in 3d space that are not\n",
    "pointing in the same direction what their span looks like? Their Span is the collection of all possible linear\n",
    "combinations of those 2 vectors, meaning all possible vectors you get by scaling each of the 2 of them in some way,\n",
    "and then adding them together. Turning the nobs of the scallars and following the tip of the resulting vector. It\n",
    "will trace a some kind of flat sheet, cutting through the origin of the 3d space. This is the span of the vectors.\n",
    "\n",
    "Two different things happen if we add a 3rd vector and consider the span of all the 3 vectors:\n",
    "\n",
    "1) if the 3rd vector happens to be sitting on the Span of the first 2, then the Span doesn't change and you are \n",
    "trapped in the same flat sheet.\n",
    "\n",
    "2) if the 3rd vector is pointing in a separate direction it unlocks access to every possible 3d vector.\n",
    "As you scale the 3rd vector it moves around that Span sheet of the 1st two, sweeping it through all space.\n",
    "You are making full use of the 3 freely changing scalars that you have at your disposal.\n",
    "\n",
    "Whenever you have multiple vectors and you are able to remove one of the them without reducing the Span, we are\n",
    "saying that they are linearly dependant. One of the vectors can be expressed as linear combination of the others\n",
    "since it is already in the Span of the others.\n",
    "\n",
    "On the other hand if each vector really does add another dimention to the Span, we say that they are linearly \n",
    "independant.\n",
    "\n",
    "Now we can formaly define basis as:\n",
    "The basis of a vector space is a set of linearly independant vectors that Span the full space.\n",
    "\n",
    "Linear independance, two definitions:\n",
    "\n",
    "1) The vectors $ \\vec{w}, \\vec{v}, \\vec{u} $ are linearly indipendant if the only solution to:\n",
    "\n",
    "$$  a\\vec{w} + b\\vec{v} + c\\vec{u} = 0 $$\n",
    "\n",
    "is\n",
    "\n",
    "$$ a = b = c = 0 $$ \n",
    "\n",
    "2) The vectors $ \\vec{w}, \\vec{v}, \\vec{u} $ are linearly indipendant if one of them is outside the span of the other two:\n",
    "\n",
    "$$ a\\vec{w} + b\\vec{v} \\neq \\vec{u} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations and Matrices\n",
    "\n",
    "The idea of linear transformation and its relation to matrices is the most important for making things click.\n",
    "It is a way of thinking about matrix vector multiplication that doesn't rely on memorization.\n",
    "\n",
    "Transformation is essentially a fancy word for function. It is something that takes inputs and returns outputs for \n",
    "each one. In the context of linear algebra we want to think of transformations as taking a vector and returning \n",
    "another vector, but transformation inplies movement and it is a great way to understand functions of vectors.\n",
    "\n",
    "If transformation takes some input vector to some output vector, we imagine that input vector moving over to the \n",
    "output. Then to understand the transformation as a whole we might imagine every possible input vector move over\n",
    "to its corresponding output vetor. And here again we just think of the point at the tip of the vector. So we watch\n",
    "every point in space move to some other point. When we talk about a 2d transformation we may represent those points\n",
    "as infinite grid transforming in space and keep the old grid behind for reference.\n",
    "\n",
    "Arbitrary transsformations tough beautiful migth be just too complex. Linear algebra deals only with linear\n",
    "transformations. Visually speaking, a transformation is linear if it has two properties:\n",
    "\n",
    "1) all lines must remain lines (without getting curved, includiing diagonal lines)\n",
    "\n",
    "2) the origin must remain fixed in place\n",
    "\n",
    "Generally it keeps lines parallel and evenly spaced.\n",
    "\n",
    "To make it work you only need to record where the basis vetors land and everithing else will follow from that.\n",
    "\n",
    "For exmample:\n",
    "\n",
    "$ \\vec{v} = \\begin{bmatrix} -1\\\\ 2\\end{bmatrix} \\text{ , meaning that: } \\vec{v} = -1\\hat{i} + 2\\hat{j} $\n",
    "\n",
    "$[ \\vec{v}, \\text{ Transformed } (\\hat{i}), \\text{ Transformed } (\\hat{j}) ] $\n",
    "\n",
    "If we play some transformation and follow where all 3 of these vectors go the property that grid lines remain \n",
    "parallel and evenly spaced will have some important consequence:\n",
    "\n",
    "The place where $ \\vec{v} $ lands will be -1 times the vector where $ \\hat{i} $ landed + 2 times the vector where\n",
    "$ \\hat{j} $ landed. In other words it started as a linear combination of $ \\hat{i} $ and $ \\hat{j} $ and it ends\n",
    "up as that same linear combination of where those two vectors landed. This means you can deduce where $ \\vec{v} $\n",
    "must go based only on where $ \\hat{i} $ and $ \\hat{j} $ each land.\n",
    "\n",
    "Transformed $ \\hat{i} $ landed on $ \\begin{bmatrix} 1\\\\ -2\\end{bmatrix} $\n",
    "\n",
    "Transformed $ \\hat{j} $ landed on $ \\begin{bmatrix} 3\\\\ 0\\end{bmatrix} $\n",
    "\n",
    "$ \\text{ Transformed } \\vec{v} = -1(\\text{ Transformed } \\hat{i}) + 2(\\text{ Transformed } \\hat{j}) = -1\\begin{bmatrix} 1\\\\ -2\\end{bmatrix} + 2\\begin{bmatrix} 3\\\\ 0\\end{bmatrix} = \\begin{bmatrix} 5\\\\ 2\\end{bmatrix} $\n",
    "\n",
    "A 2d linear trnsformation can be described by just 4 numbers, the coordinates for where the basis vectors land.\n",
    "It is common to package those numbers into a 2 by 2 grid of numbers called a 2 x 2 matrix, where you can interpret\n",
    "the columns as the 2 special vectors where the $ \\hat{i} $ and $ \\hat{j} $ lands.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    3 & -2 \\\\\n",
    "    2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Union of sets A and B $ A \\cup B  $\n",
    "\n",
    "$$ x \\in A \\cup B \\text{ iff } x \\in A \\text{ OR } x \\in B $$\n",
    "\n",
    "$$ f : A \\to B $$\n",
    "$$ f(P, Q) = [P \\text{ IMPLIES } Q]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Products and Duality\n",
    "\n",
    "\"Only in the light of linear transformations we can fully understand what a dot product is.\"\n",
    "\n",
    "* Formal representation\n",
    "\n",
    "Numericaly if you have 2 vectors of the same dimention, taking their dot product means\n",
    "pairing up all the coordinates, multipling those pairs together and adding the result.\n",
    "\n",
    "Like mapping over the length with multiplication and then reducing with addition to squish it down to a single number on the number line.\n",
    "\n",
    "$$ \\begin{bmatrix} 2\\\\ 7\\\\ 1\\end{bmatrix} \\cdot \\begin{bmatrix} 8\\\\ 2\\\\ 8\\end{bmatrix} = \\begin{matrix} 2 \\cdot 8\\\\ +\\\\ 7 \\cdot 2\\\\ + \\\\ 1 \\cdot 8 \\end{matrix} = 38 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v⃗ = [2;7;1];\n",
    "w⃗ = [8;2;8];\n",
    "\n",
    "p1 = dot(v⃗, w⃗)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graphical interpretation\n",
    "\n",
    "To think about the dot product between two vectors $ \\vec{v} $ and $ \\vec{w} $, imagine\n",
    "projecting $ \\vec{w} $ onto the line that passes through the origin and the tip of $ \\vec{v} $.\n",
    "Multiply the length of that projection by the length of $ \\vec{v} $ and you get the dot product\n",
    "$ \\vec{v} $ and $ \\vec{w} $. If tha projection of $ \\vec{w} $ is pointing in the opposite direction\n",
    "from $ \\vec{v} $ the dot product will be negative. If they are penpendicular their dot product is 0.\n",
    "\n",
    "$$ \\vec{v} \\cdot \\vec{w} = \\begin{bmatrix} 4\\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} -2\\\\ 1 \\end{bmatrix} = (\\text{Length  of  projected } \\vec{w})(\\text{Length of } \\vec{v}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpretation seems asymetric, it treats the two vector differently. But actualy order still doesn't matter.\n",
    "If $ \\vec{v} $ and $ \\vec{w} $ happened to have the same length we could levarege some symmetry.\n",
    "Since projecting $ \\vec{v} $ onto $ \\vec{w} $ than multiplying the length of that projection by the length\n",
    "of $ \\vec{v} $, is a complete mirror image of projecting $ \\vec{v} $ onto $ \\vec{w} $ than multipling that\n",
    "projection by the length of $ \\vec{w} $.\n",
    "\n",
    "Now if you scale one of them, say $ \\vec{v} $ by some constant like 2, so that they don't have equal length,\n",
    "the symmetry is broken. But lets think how to interprete the dot product between this new vector $ 2\\vec{v} $\n",
    "and $ \\vec{w} $. If you think of $ \\vec{w} $ as getting projected onto $ \\vec{v} $ then the dot product will\n",
    "be exatly twice the dot product $ \\vec{v}.\\vec{w} $. This is because when you scale $ \\vec{v} $ by 2 it\n",
    "doesn't change the length of the projection of $ \\vec{w} $, but it doubles the length of the vector that\n",
    "you are projecting onto. \n",
    "\n",
    "$$ (2\\vec{v}) \\cdot \\vec{w} = 2(\\vec{v} \\cdot \\vec{w})  $$\n",
    "\n",
    "![caption](img/1806/projection_symmetry_1.jpg)\n",
    "\n",
    "But on the other hand lets think of $ \\vec{v} $ getting projected onto $ \\vec{w} $. In that case the length\n",
    "of the projection is the thing to get scaled we multiply  $ \\vec{v} $ by 2. The length of the vector you are projecting onto stays constant. So the overall effect is still to just double the dot product. So even the\n",
    "symmetry is broken the effect that this scaling have on the dot product is the same under both interpretations.\n",
    "\n",
    "![caption](img/1806/projection_symmetry_full.jpg)\n",
    "\n",
    "Why both views are connected?\n",
    "\n",
    "Why the computaion have anything to do with the projection?\n",
    "\n",
    "The answer is in something deeper known as \"Duality\". Linear transformations from multiple dimentions to just\n",
    "one dimention (the number line) are important here. Fuctions that take a nd vector and return a number.\n",
    "\n",
    "$$ \\begin{bmatrix} 3\\\\ 7\\end{bmatrix} \\to L(\\vec{v}) \\to \\begin{bmatrix} 1.8 \\end{bmatrix} $$\n",
    "\n",
    "We will focus on a certain visual property that is equivalent to the formal properties that make these functions\n",
    "linear.\n",
    "\n",
    "$$ L(\\vec{v} + \\vec{w}) = L(\\vec{v}) + L(\\vec{w}) $$\n",
    "\n",
    "$$ L(a\\vec{v}) = aL(\\vec{v}) $$\n",
    "\n",
    "If you take a line of evenly spaced dots and apply a transformation, a lineear transformation will keep those dots\n",
    "evenly spaced, once they land on the output space, which is the number line. Otherwise the transformation is not \n",
    "linear.\n",
    "\n",
    "$$ f \\begin{pmatrix} \\begin{bmatrix} x\\\\ y\\end{bmatrix} \\end{pmatrix} = x^{2} + y^{2} \\text{, is a nonlinear transformation } $$\n",
    "\n",
    "The linear transformation is completely determined by where $ \\hat{i} $ and $ \\hat{j} $ land.\n",
    "\n",
    "There is nice association between 1x2 matrices and 2d vectors, defined by tilting the numerical representation of a \n",
    "vector on its side to get the associated matrix, or to tip the matrix back up to get the associated vector.\n",
    "\n",
    "It seems silly, but it has some nice geometric implications. There is some kind of connection between linear \n",
    "transformations that take vectors to numbers, and vectors themselves.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Lets take a copy of the number line and place it diagonaly in 2d space with the number 0 sitting at the origin.\n",
    "- Now think of the 2d unit vector $ \\hat{u} $ whose tip sits on number 1 on that number line. It is 2d vector living in 2d space it is just overlaping with the number line.\n",
    "- If we project 2d vectors onto this diagonal line, we are defining a linear function that takes 2d vectors to numbers. \n",
    "\n",
    "![](img/1806/embeded_number_line_full.jpg)\n",
    "\n",
    "Because we have just defined a linear transformation we are going to be able to find a 1x2 matrix that describes that \n",
    "transformation. This is our projection matrix. Its columns are the the points on the embeded number line where $ \n",
    "\\hat{i} $ and $ \\hat{j} $ lands. There is beutiful symmetry. Since $ \\hat{i} $ and $ \\hat{u} $ are both unit vectors,\n",
    "projecting $ \\hat{i} $ onto the line passing through $ \\hat{u} $ looks totaly symmetric to projecting $ \\hat{u} $ \n",
    "onto the x-axis ($ \\hat{i} $). So they land onto the same number on their corresponding number lines. And since \n",
    "projecting $ \\hat{u} $ onto the x-axis just means taking the x-coordinate of $ \\hat{u} $ ($ u_x $ ).\n",
    "\n",
    "- So the entries of the 1x2 matrix, describing the projection transformation are going to be the coordinates of $  \n",
    "\\hat{u} $.\n",
    " \n",
    "- And computing this transformation for arbitrary vectors in space is computationaly identical to taking a dot product with  $ \\hat{u} $.\n",
    "\n",
    "$$ \\begin{bmatrix} u_x & u_y \\end{bmatrix} \\cdot \\begin{bmatrix} x\\\\ y \\end{bmatrix} = u_x \\cdot x + u_y \\cdot y $$\n",
    "\n",
    "This is why taking the dot product with a unit vector, can be interpreted as projecting a vector onto the span of \n",
    "that unit vector and taking the length. \n",
    "\n",
    "![symmetrical projections](img/1806/line_of_symmetry_full.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Products\n",
    "\n",
    "If you have two vectors $ \\vec{v} $ and $ \\vec{w} $ think about the parallelogram that they span out. Meaning if you\n",
    "take a copy of $ \\vec{v} $ and move its tail to the tip of $ \\vec{w} $ and you take a copy of $ \\vec{w} $ and move \n",
    "its tail to the tip of $ \\vec{v} $ the 4 vectors now enclose a certain parallelogram.\n",
    "\n",
    "$$ \\vec{v} \\times \\vec{w} = \\text{Area of parallelogram } $$\n",
    "\n",
    "But we also need to consider orientation. If $ \\vec{v} $ is on the right of $ \\vec{w} $ then $ \\vec{v} \\times \\vec{w} \n",
    "$ is positive and equal to the area of the parallelogram. But if $ \\vec{w} $ is on the left the cross product is \n",
    "negative. Note that this means that order matters. If you take the cross product of the two basis vectors the result \n",
    "should be positive. The order of the basis vectors is what defines orientation.\n",
    "\n",
    "$$ \\hat{i} \\times \\hat{j} = +1 $$\n",
    "\n",
    "Remember you can take the area by taking the determinant. You write the coordinates of \\vec{v} as the 1st colomn of \n",
    "the matrix and the coordinates of $ \\vec{w} $ as the 2nd colomn of the matrix, then you just compute the determinant.\n",
    "This is because a matrix whose colomns represent $ \\vec{v} $ and $ \\vec{w} $ corresponds with a linenear \n",
    "transformation that moves the basis vectors $ \\hat{i}$ and $ \\hat{j} $ to $ \\vec{v} $ and $ \\vec{w} $. The determinat \n",
    "is all about measuring how areas change due to a transformation and the prototypical area that we look at is the unit \n",
    "square. After the transformation that square gets turned into that parallelogram.\n",
    "\n",
    "$$ \\begin{bmatrix} 3\\\\ 1\\\\ \\end{bmatrix} \\times \\begin{bmatrix} 2\\\\ -1\\\\ \\end{bmatrix} = \\text{det}\\begin{pmatrix} \\begin{bmatrix} 3 & 2 \\\\ 1 & -1\\end{bmatrix} \\end{pmatrix}$$\n",
    "\n",
    "Notice that when two vectors are perpendicular or close to being perpendicular their is larger than it would be if \n",
    "they were pointing in very similar directions. Something else is that if you scaled one of the vectors, lets say by 3 \n",
    "the area also is scaled up by a factor of 3.\n",
    "\n",
    "$$ (3\\vec{v}) \\times \\vec{w} = 3(\\vec{v} \\times \\vec{w}) $$\n",
    "\n",
    "The true cross product(3d space) is something that combines two different 3d vectors to get a new 3d vector. As \n",
    "before we still consider the parallelogram and its area still plays a role. But the cross product is not a number it \n",
    "is a vector. This new vectors' length will be the area of that parallelogram and the direction of this new vector is \n",
    "going to be perpendicular to the parallelogram.\n",
    "\n",
    "There is a formula to make the computation:\n",
    "\n",
    "$$ \\begin{bmatrix}v_1 \\\\v_2 \\\\v_3\\end{bmatrix} \\cdot \\begin{bmatrix}w_1 \\\\w_2 \\\\w_3\\end{bmatrix} = \\begin{bmatrix}v_2 \\cdot w_3 - w_2 \\cdot v_3 \\\\ v_3 \\cdot w_1 - w_3 \\cdot v_1\\\\ v_1 \\cdot w_2 - w_1 \\cdot v_2 \\end{bmatrix}$$\n",
    "\n",
    "But there is also a process:\n",
    "\n",
    "First you write down the 3d matrix, where the 2nd and 3rd colomns contain the coordinates of $ \\vec{v} $ and $ \n",
    "\\vec{w} $. But for the 1st colomn you write the basis vectors $ \\hat{i} $ and $ \\hat{j} $ and  $ \\hat{k} $. Then you \n",
    "compute the determinant of this matrix.\n",
    "\n",
    "$$ \\begin{bmatrix}v_1 \\\\v_2 \\\\v_3\\end{bmatrix} \\cdot \\begin{bmatrix}w_1 \\\\w_2 \\\\w_3\\end{bmatrix} = \\text{det}\\begin{pmatrix} \\begin{bmatrix} \\hat{i} & v_1 & w_1 \\\\ \\hat{j} & v_2 & w_2 \\\\ \\hat{k} & v_3 & w_3 \\end{bmatrix} \\end{pmatrix}$$\n",
    "\n",
    "$$ \\hat{i}(v_2 \\cdot w_3 - v_3 \\cdot w_2) + \\hat{j}(w_3 \\cdot w_1 - v_1 \\cdot w_3) + \\hat{k}(v_1 \\cdot w_2 - v_2 \\cdot w_1) $$\n",
    "\n",
    "When you carry out the computation as if $ \\hat{i} $ and $ \\hat{j} $ and  $ \\hat{k} $ were simply numbers then you \n",
    "get some linear combination of those basis vectors. The result is the unique vector that is perpendicular to the \n",
    "parallelogram and its magnitude is the area of the parallelogram, and its direction obeys the right hand rule.\n",
    "\n",
    "![caption](img/1806/right_hand_rule.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Products and Duality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{bmatrix} \\\\ \\\\ \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
